{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'../data/olhs_run1.xlsx'\n",
    "x_pd = pd.read_excel(filename, sheet_name='Initial Design (OLHS)', header=[0,1], index_col=[0])\n",
    "y_pd = pd.read_excel(filename, sheet_name='bo_data', header=[0,1], index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th>Group ID</th>\n",
       "      <th>Polymer Solubility</th>\n",
       "      <th>Gelation Enthalpy</th>\n",
       "      <th>Shear Modulus</th>\n",
       "      <th>Manufacturability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <th>mg/mL</th>\n",
       "      <th>J/g</th>\n",
       "      <th>Kpa</th>\n",
       "      <th>--</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>51.344743</td>\n",
       "      <td>1.25180</td>\n",
       "      <td>4.965500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20.121238</td>\n",
       "      <td>56.63540</td>\n",
       "      <td>0.536800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35.672397</td>\n",
       "      <td>3.46590</td>\n",
       "      <td>1.036200</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29.114323</td>\n",
       "      <td>2.56890</td>\n",
       "      <td>0.998800</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>123.417722</td>\n",
       "      <td>0.14074</td>\n",
       "      <td>1.899265</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>81.196581</td>\n",
       "      <td>88.70100</td>\n",
       "      <td>1.078000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>84.919473</td>\n",
       "      <td>0.30367</td>\n",
       "      <td>2.343650</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>64.625850</td>\n",
       "      <td>27.80800</td>\n",
       "      <td>3.263690</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>47.923323</td>\n",
       "      <td>116.06000</td>\n",
       "      <td>5.088030</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>173.697270</td>\n",
       "      <td>2.05890</td>\n",
       "      <td>10.186500</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>47.904192</td>\n",
       "      <td>122.74000</td>\n",
       "      <td>0.285000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>103.370787</td>\n",
       "      <td>16.75100</td>\n",
       "      <td>1.845836</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>80.965909</td>\n",
       "      <td>7.85260</td>\n",
       "      <td>9.460780</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>63.909774</td>\n",
       "      <td>114.01000</td>\n",
       "      <td>41.827700</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>35.087719</td>\n",
       "      <td>45.47800</td>\n",
       "      <td>0.880955</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>120.218579</td>\n",
       "      <td>1.47510</td>\n",
       "      <td>9.700660</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>152.439024</td>\n",
       "      <td>15.30000</td>\n",
       "      <td>29.505300</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>79.096045</td>\n",
       "      <td>29.05000</td>\n",
       "      <td>15.751700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>116.319444</td>\n",
       "      <td>0.47714</td>\n",
       "      <td>0.030895</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>57.750760</td>\n",
       "      <td>1.65600</td>\n",
       "      <td>0.180700</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Group ID Polymer Solubility Gelation Enthalpy Shear Modulus Manufacturability\n",
       "#                     mg/mL               J/g           Kpa                --\n",
       "1                 51.344743           1.25180      4.965500                 1\n",
       "2                 20.121238          56.63540      0.536800                 0\n",
       "3                 35.672397           3.46590      1.036200                 0\n",
       "4                 29.114323           2.56890      0.998800                 0\n",
       "5                123.417722           0.14074      1.899265                 1\n",
       "6                 81.196581          88.70100      1.078000                 1\n",
       "7                 84.919473           0.30367      2.343650                 1\n",
       "8                 64.625850          27.80800      3.263690                 1\n",
       "9                 47.923323         116.06000      5.088030                 1\n",
       "10               173.697270           2.05890     10.186500                 1\n",
       "11                47.904192         122.74000      0.285000                 1\n",
       "12               103.370787          16.75100      1.845836                 1\n",
       "13                80.965909           7.85260      9.460780                 1\n",
       "14                63.909774         114.01000     41.827700                 1\n",
       "15                35.087719          45.47800      0.880955                 0\n",
       "16               120.218579           1.47510      9.700660                 1\n",
       "17               152.439024          15.30000     29.505300                 1\n",
       "18                79.096045          29.05000     15.751700                 0\n",
       "19               116.319444           0.47714      0.030895                 1\n",
       "20                57.750760           1.65600      0.180700                 0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the inputs\n",
    "xmeans = x_pd.mean(axis=0)\n",
    "xstddv = x_pd.std(axis=0)\n",
    "x_pd_normal = (x_pd - xmeans)/xstddv\n",
    "\n",
    "x_pd = x_pd_normal\n",
    "\n",
    "x_mins = x_pd.min(axis=0).to_numpy()\n",
    "x_maxs = x_pd.max(axis=0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize objective value data\n",
    "y_obj_pd = y_pd.iloc[:, [0,1,2]]\n",
    "ymeans = y_obj_pd.mean(axis=0)\n",
    "ystddv = y_obj_pd.std(axis=0)\n",
    "y_obj_pd_normal = (y_obj_pd - ymeans) / ystddv\n",
    "\n",
    "y_pd.iloc[:, [0,1,2]] = y_obj_pd_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_idx = [1,7,15]\n",
    "\n",
    "train_x_pd = x_pd.drop(validation_idx)\n",
    "train_y_pd = y_pd.drop(validation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make torch tensors\n",
    "train_x = torch.tensor(train_x_pd.values, dtype=torch.float)\n",
    "train_y1 = torch.tensor(train_y_pd['Polymer Solubility', 'mg/mL'].values, dtype=torch.float).squeeze()\n",
    "train_y2 = torch.tensor(train_y_pd['Gelation Enthalpy', 'J/g'].values, dtype=torch.float).squeeze()\n",
    "train_y3 = torch.tensor(train_y_pd['Shear Modulus', 'Kpa'].values, dtype=torch.float).squeeze()\n",
    "train_y4 = torch.tensor(train_y_pd['Manufacturability', '--'].values, dtype=torch.long).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.tensor(x_pd.values, dtype=torch.float)\n",
    "test_y = torch.tensor(y_pd.values, dtype=torch.float).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "# function to optimize parameters of the regression GP -\n",
    "def train_reg_gp(model, likelihood, train_x, train_y, training_iter):\n",
    "   # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        if i - 1  == training_iter:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                model.covar_module.base_kernel.lengthscale.item(),\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# function to optimize parameters of the classification GP - \n",
    "def train_cls_gp(model, likelihood, train_x, training_iter):\n",
    "   # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define likelihood and initialize model - regression\n",
    "reg_likl = gpytorch.likelihoods.GaussianLikelihood()\n",
    "reg_model1 = ExactGPModel(train_x, train_y1, reg_likl)\n",
    "reg_model2 = ExactGPModel(train_x, train_y2, reg_likl)\n",
    "reg_model3 = ExactGPModel(train_x, train_y3, reg_likl)\n",
    "\n",
    "#define likelihood and initialize model - classification\n",
    "cls_likl = gpytorch.likelihoods.DirichletClassificationLikelihood(train_y4, learn_additional_noise=False, alpha_epsilon=1e-4)\n",
    "cls_model = DirichletGPModel(train_x, cls_likl.transformed_targets, cls_likl, num_classes=cls_likl.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 50\n",
    "\n",
    "#train model - regression\n",
    "reg_model1, reg_likl1 = train_reg_gp(reg_model1, reg_likl, train_x, train_y1, training_iter)\n",
    "reg_model2, reg_likl2 = train_reg_gp(reg_model2, reg_likl, train_x, train_y2, training_iter)\n",
    "reg_model3, reg_likl3 = train_reg_gp(reg_model3, reg_likl, train_x, train_y3, training_iter)\n",
    "\n",
    "#train model - regression\n",
    "cls_model, cls_likl = train_cls_gp(cls_model, cls_likl, train_x, training_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, likl, X):\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likl(model(X))\n",
    "        mean = pred.mean\n",
    "        std = pred.stddev\n",
    "    return mean, std\n",
    "\n",
    "#sum the available models to convert mutli obj problem to single obj\n",
    "def multi_objective_predict(X, models, liklihoods):\n",
    "    mean = 0\n",
    "    var = 0\n",
    "    for i in range(len(models)):\n",
    "        tmp_mean, tmp_std = predict(models[i], liklihoods[i], torch.tensor(X, dtype=torch.float))\n",
    "        mean += tmp_mean\n",
    "        var += tmp_std**2\n",
    "    return mean, var**0.5\n",
    "\n",
    "# function to calculate acquisition function \n",
    "def acquisition_func(X, models, liklihoods, y_min_curr):\n",
    "    X = np.reshape(X, (1, -1))\n",
    "    pred_mean, pred_std = multi_objective_predict(X, models, liklihoods) \n",
    "    improv = pred_mean - y_min_curr\n",
    "    z_score = np.divide(improv, pred_std + 1E-9)\n",
    "    acf = np.multiply(improv, norm.cdf(z_score)) + np.multiply(pred_std, norm.pdf(z_score))\n",
    "    return (-1.0) * acf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for best point\n",
    "n_restarts = 100\n",
    "# x_bounds = np.array([[2000, 10000], [0, 100], [0, 40], [5000, 15000], [80, 100], [0,100], [60, 100], [70, 100]])\n",
    "x_bounds = np.c_[x_mins, x_maxs]\n",
    "search_grid = np.random.uniform(x_bounds[:, 0], x_bounds[:, 1], size=(10*n_restarts, len(x_bounds)))\n",
    "\n",
    "mo_models = [reg_model1, reg_model2]\n",
    "mo_likls = [reg_likl1, reg_likl2]\n",
    "\n",
    "# Swith on eval mode\n",
    "for i in range(len(mo_models)):\n",
    "    mo_models[i].eval()\n",
    "    mo_likls[i].eval()\n",
    "\n",
    "y_best_curr = torch.max((train_y1 + train_y2)).item()\n",
    "\n",
    "acf_vals = [acquisition_func(search_grid[i, :].reshape(1, -1), mo_models, mo_likls, y_best_curr) for i in range(10*n_restarts)]\n",
    "\n",
    "acf_vals = np.array(acf_vals).reshape(10*n_restarts,)\n",
    "top_idx = np.argsort(acf_vals)\n",
    "search_grid = search_grid[top_idx[0:n_restarts]].reshape(n_restarts,-1)\n",
    "\n",
    "best_acquisition_values = 1\n",
    "best_x = None\n",
    "for i, starting_point in enumerate(search_grid):\n",
    "    res = minimize(fun=acquisition_func, \n",
    "                   x0=starting_point, \n",
    "                   bounds=x_bounds,\n",
    "                   method='L-BFGS-B',\n",
    "                   args=(mo_models, mo_likls, y_best_curr))\n",
    "    if res.fun < best_acquisition_values:\n",
    "        best_acquisition_values = res.fun\n",
    "        best_x = res.x\n",
    "\n",
    "new_point = best_x\n",
    "new_acf = (-1.0) * best_acquisition_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inverse transform \n",
    "def inverse_transform(val, mean, std):\n",
    "    return std * val + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted output at new point -  tensor([2.4799])\n",
      "std deviation -  tensor([1.1549])\n"
     ]
    }
   ],
   "source": [
    "pred, std = multi_objective_predict(new_point.reshape(1,-1), mo_models, mo_likls)\n",
    "print('Predicted output at new point - ', pred)\n",
    "print('std deviation - ', std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2.4151])  and  tensor([0.0647])\n"
     ]
    }
   ],
   "source": [
    "new_pt_tensor = torch.tensor(new_point.reshape(1,-1), dtype=torch.float)\n",
    "y1_pred, _ = predict(reg_model1, reg_likl1, new_pt_tensor)\n",
    "y2_pred, _ = predict(reg_model2, reg_likl2, new_pt_tensor)\n",
    "print(y1_pred, ' and ', y2_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.22111523, -1.14006761, -0.46216723,  1.24853254,  0.43425369,\n",
       "        0.17214098, -1.4405752 ,  1.41954212])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([179.7163])\n"
     ]
    }
   ],
   "source": [
    "y1_pred = inverse_transform(y1_pred, ymeans.values[0], ystddv.values[0])\n",
    "y2_pred = inverse_transform(y2_pred, ymeans.values[1], ystddv.values[1])\n",
    "print(y1_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "143.9980154794394"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inverse_transform(y_best_curr, ymeans.values[0], ystddv.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[9041.774785340262,\n",
       " 14.50141610594094,\n",
       " 14.243751126584717,\n",
       " 13887.588474175576,\n",
       " 92.70429419988953,\n",
       " 55.35999886869985,\n",
       " 62.05777298547319,\n",
       " 98.26019646051431]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pt_tr = [inverse_transform(new_point[i], xmeans.values[i], xstddv.values[i]) for i in range(len(new_point))]\n",
    "new_pt_tr"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
