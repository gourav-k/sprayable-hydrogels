{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'../data/olhs_run1.xlsx'\n",
    "x_pd = pd.read_excel(filename, sheet_name='Initial Design (OLHS)', header=[0,1], index_col=[0])\n",
    "y_pd = pd.read_excel(filename, sheet_name='bo_data', header=[0,1], index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the inputs\n",
    "xmeans = x_pd.mean(axis=0)\n",
    "xstddv = x_pd.std(axis=0)\n",
    "x_pd_normal = (x_pd - xmeans)/xstddv\n",
    "\n",
    "x_pd = x_pd_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize objective value data\n",
    "y_obj_pd = y_pd.iloc[:, [0,1,2]]\n",
    "ymeans = y_obj_pd.mean(axis=0)\n",
    "ystddv = y_obj_pd.std(axis=0)\n",
    "y_obj_pd_normal = (y_obj_pd - ymeans) / ystddv\n",
    "\n",
    "y_pd.iloc[:, [0,1,2]] = y_obj_pd_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_idx = [1,7,15]\n",
    "\n",
    "train_x_pd = x_pd.drop(validation_idx)\n",
    "train_y_pd = y_pd.drop(validation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make torch tensors\n",
    "train_x = torch.tensor(train_x_pd.values, dtype=torch.float)\n",
    "train_y1 = torch.tensor(train_y_pd['Polymer Solubility', 'mg/mL'].values, dtype=torch.float).squeeze()\n",
    "train_y2 = torch.tensor(train_y_pd['Gelation Enthalpy', 'J/g'].values, dtype=torch.float).squeeze()\n",
    "train_y3 = torch.tensor(train_y_pd['Shear Modulus', 'Kpa'].values, dtype=torch.float).squeeze()\n",
    "train_y4 = torch.tensor(train_y_pd['Manufacturability', '--'].values, dtype=torch.long).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.tensor(x_pd.values, dtype=torch.float)\n",
    "test_y = torch.tensor(y_pd.values, dtype=torch.float).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "# function to optimize parameters of the regression GP -\n",
    "def train_reg_gp(model, likelihood, train_x, train_y, training_iter):\n",
    "   # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        if i - 1  == training_iter:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                model.covar_module.base_kernel.lengthscale.item(),\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# function to optimize parameters of the classification GP - \n",
    "def train_cls_gp(model, likelihood, train_x, training_iter):\n",
    "   # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define likelihood and initialize model - regression\n",
    "reg_likl = gpytorch.likelihoods.GaussianLikelihood()\n",
    "reg_model1 = ExactGPModel(train_x, train_y1, reg_likl)\n",
    "reg_model2 = ExactGPModel(train_x, train_y2, reg_likl)\n",
    "reg_model3 = ExactGPModel(train_x, train_y3, reg_likl)\n",
    "\n",
    "#define likelihood and initialize model - classification\n",
    "cls_likl = gpytorch.likelihoods.DirichletClassificationLikelihood(train_y4, learn_additional_noise=False, alpha_epsilon=1e-4)\n",
    "cls_model = DirichletGPModel(train_x, cls_likl.transformed_targets, cls_likl, num_classes=cls_likl.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 50\n",
    "\n",
    "#train model - regression\n",
    "reg_model1, reg_likl1 = train_reg_gp(reg_model1, reg_likl, train_x, train_y1, training_iter)\n",
    "reg_model2, reg_likl2 = train_reg_gp(reg_model2, reg_likl, train_x, train_y2, training_iter)\n",
    "reg_model3, reg_likl3 = train_reg_gp(reg_model3, reg_likl, train_x, train_y3, training_iter)\n",
    "\n",
    "#train model - regression\n",
    "cls_model, cls_likl = train_cls_gp(cls_model, cls_likl, train_x, training_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, likl, X):\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likl(model(X))\n",
    "        mean = pred.mean\n",
    "        std = pred.stddev\n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#sum the available models to convert mutli obj problem to single obj\n",
    "def multi_objective_predict(X, models, liklihoods):\n",
    "    mean = 0\n",
    "    var = 0\n",
    "    for model in models:\n",
    "        tmp_mean, tmp_std = predict(model, liklihoods[0], torch.tensor(X, dtype=torch.float))\n",
    "        mean += tmp_mean\n",
    "        var += tmp_std**2\n",
    "    return mean, var**0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to calculate acquisition function \n",
    "def acquisition_func(X, models, liklihoods, y_min_curr):\n",
    "    X = np.reshape(X, (1, -1))\n",
    "    pred_mean, pred_std = multi_objective_predict(X, models, liklihoods) \n",
    "    improv = -y_min_curr + pred_mean\n",
    "    z_score = np.divide(improv, pred_std + 1E-9)\n",
    "    acf = np.multiply(improv, norm.cdf(z_score)) + np.multiply(pred_std, norm.pdf(z_score))\n",
    "    return (-1.0) * acf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search for best point\n",
    "\n",
    "x_bounds = np.array([[2000, 10000], [0, 100], [0, 40], [5000, 15000], [80, 100], [0,100], [60, 100], [70, 100]])\n",
    "search_grid = np.random.uniform(x_bounds[:, 0], x_bounds[:, 1], size=(100, len(x_bounds)))\n",
    "\n",
    "mo_models = [reg_model1, reg_model2]\n",
    "mo_likls = [reg_likl1, reg_likl2]\n",
    "\n",
    "reg_model1.eval()\n",
    "reg_model2.eval()\n",
    "reg_likl1.eval()\n",
    "reg_likl2.eval()\n",
    "\n",
    "y_best_curr = torch.max(train_y1 + train_y2).item()\n",
    "\n",
    "acf_vals = [acquisition_func(search_grid[i, :].reshape(1, -1), mo_models, mo_likls, y_best_curr) for i in range(100)]\\\n",
    "\n",
    "acf_vals = np.array(acf_vals).reshape(100,)\n",
    "top_idx = np.argsort(acf_vals)\n",
    "search_grid = search_grid[top_idx[0:10]].reshape(10,-1)\n",
    "\n",
    "best_acquisition_values = 1\n",
    "best_x = None\n",
    "for i, starting_point in enumerate(search_grid):\n",
    "    res = minimize(fun=acquisition_func, \n",
    "                   x0=starting_point, \n",
    "                   bounds=x_bounds,\n",
    "                   method='L-BFGS-B',\n",
    "                   args=(mo_models, mo_likls, y_best_curr))\n",
    "    if res.fun < best_acquisition_values:\n",
    "        best_acquisition_values = res.fun\n",
    "        best_x = res.x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\goura\\AppData\\Local\\Temp\\ipykernel_14908\\4118473385.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tmp_mean, tmp_std = predict(model, liklihoods[0], torch.tensor(X, dtype=torch.float))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([0.0801]), tensor([1.7745]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_x = torch.tensor(best_x.reshape(1,-1), dtype=torch.float)\n",
    "multi_objective_predict(best_x, mo_models, mo_likls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\goura\\anaconda3\\envs\\gpytorch\\Lib\\site-packages\\gpytorch\\likelihoods\\gaussian_likelihood.py:300: GPInputWarning: You have passed data through a FixedNoiseGaussianLikelihood that did not match the size of the fixed noise, *and* you did not specify noise. This is treated as a no-op.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(tensor([[-3.2414],\n",
       "         [-2.1741]]),\n",
       " tensor([[2.2303],\n",
       "         [2.3356]]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_model.eval()\n",
    "cls_likl.eval()\n",
    "predict(cls_model, cls_likl, best_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2.6837e+03, 8.3361e+01, 1.4722e+00, 1.3654e+04, 8.6976e+01, 4.1206e+01,\n",
       "         6.6321e+01, 8.0358e+01]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
