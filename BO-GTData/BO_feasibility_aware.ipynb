{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import gpytorch\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import random\n",
    "from scipy.stats import norm\n",
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'../data/olhs_run1.xlsx'\n",
    "x_pd = pd.read_excel(filename, sheet_name='Initial Design (OLHS)', header=[0,1], index_col=[0])\n",
    "y_pd = pd.read_excel(filename, sheet_name='bo_data', header=[0,1], index_col=[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizing the inputs\n",
    "xmeans = x_pd.mean(axis=0)\n",
    "xstddv = x_pd.std(axis=0)\n",
    "x_pd_normal = (x_pd - xmeans)/xstddv\n",
    "\n",
    "x_pd = x_pd_normal\n",
    "\n",
    "x_mins = x_pd.min(axis=0).to_numpy()\n",
    "x_maxs = x_pd.max(axis=0).to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalize objective value data\n",
    "y_obj_pd = y_pd.iloc[:, [0,1,2]]\n",
    "ymeans = y_obj_pd.mean(axis=0)\n",
    "ystddv = y_obj_pd.std(axis=0)\n",
    "y_obj_pd_normal = (y_obj_pd - ymeans) / ystddv\n",
    "\n",
    "y_pd.iloc[:, [0,1,2]] = y_obj_pd_normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "objective_properties = ['Polymer Solubility', 'Gelation Enthalpy', 'Shear Modulus']\n",
    "# objective_properties = ['Polymer Solubility', 'Gelation Enthalpy']\n",
    "scaling = {'ymeans': ymeans[objective_properties],\n",
    "           'ystddv': ystddv[objective_properties],\n",
    "           'xmeans': xmeans,\n",
    "           'xstddv': xstddv}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_idx = [1,7,15]\n",
    "\n",
    "train_x_pd = x_pd.drop(validation_idx)\n",
    "train_y_pd = y_pd.drop(validation_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#make torch tensors\n",
    "train_x = torch.tensor(train_x_pd.values, dtype=torch.float)\n",
    "train_y1 = torch.tensor(train_y_pd['Polymer Solubility', 'mg/mL'].values, dtype=torch.float).squeeze()\n",
    "train_y2 = torch.tensor(train_y_pd['Gelation Enthalpy', 'J/g'].values, dtype=torch.float).squeeze()\n",
    "train_y3 = torch.tensor(train_y_pd['Shear Modulus', 'Kpa'].values, dtype=torch.float).squeeze()\n",
    "train_y4 = torch.tensor(train_y_pd['Manufacturability', '--'].values, dtype=torch.long).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = torch.tensor(x_pd.values, dtype=torch.float)\n",
    "test_y = torch.tensor(y_pd.values, dtype=torch.float).squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "    \n",
    "# function to optimize parameters of the regression GP -\n",
    "def train_reg_gp(model, likelihood, train_x, train_y, training_iter):\n",
    "   # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        if i - 1  == training_iter:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                model.covar_module.base_kernel.lengthscale.item(),\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DirichletGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# function to optimize parameters of the classification GP - \n",
    "def train_cls_gp(model, likelihood, train_x, training_iter):\n",
    "   # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define likelihood and initialize model - regression\n",
    "reg_likl = gpytorch.likelihoods.GaussianLikelihood()\n",
    "reg_model1 = ExactGPModel(train_x, train_y1, reg_likl)\n",
    "reg_model2 = ExactGPModel(train_x, train_y2, reg_likl)\n",
    "reg_model3 = ExactGPModel(train_x, train_y3, reg_likl)\n",
    "\n",
    "#define likelihood and initialize model - classification\n",
    "cls_likl = gpytorch.likelihoods.DirichletClassificationLikelihood(train_y4, learn_additional_noise=False, alpha_epsilon=1e-4)\n",
    "cls_model = DirichletGPModel(train_x, cls_likl.transformed_targets, cls_likl, num_classes=cls_likl.num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_iter = 50\n",
    "\n",
    "#train model - regression\n",
    "reg_model1, reg_likl1 = train_reg_gp(reg_model1, reg_likl, train_x, train_y1, training_iter)\n",
    "reg_model2, reg_likl2 = train_reg_gp(reg_model2, reg_likl, train_x, train_y2, training_iter)\n",
    "reg_model3, reg_likl3 = train_reg_gp(reg_model3, reg_likl, train_x, train_y3, training_iter)\n",
    "\n",
    "#train model - regression\n",
    "cls_model, cls_likl = train_cls_gp(cls_model, cls_likl, train_x, training_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, likl, X):\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        pred = likl(model(X))\n",
    "        mean = pred.mean\n",
    "        std = pred.stddev\n",
    "    return mean, std\n",
    "\n",
    "#sum the available models to convert mutli obj problem to single obj\n",
    "def multi_objective_predict(X, models, liklihoods):\n",
    "    mean = 0\n",
    "    var = 0\n",
    "    for i in range(len(models)):\n",
    "        tmp_mean, tmp_std = predict(models[i], liklihoods[i], torch.tensor(X, dtype=torch.float))\n",
    "        mean += tmp_mean\n",
    "        var += tmp_std**2\n",
    "    return mean, var**0.5\n",
    "\n",
    "# function to calculate acquisition function - feasibility weighted acquisition\n",
    "def acquisition_func(X, models, liklihoods, y_min_curr):\n",
    "    X = np.reshape(X, (1, -1))\n",
    "    pred_mean, pred_std = multi_objective_predict(X, models[0:-1], liklihoods[0:-1]) #get prediction mean and standard dev\n",
    "    cls_probs = class_probability(X, models[-1]) # vector with class probabilities prediction\n",
    "    feasible_probs = cls_probs[1] #class 1 is assumed as feasible class\n",
    "\n",
    "    improv = pred_mean - y_min_curr\n",
    "    z_score = np.divide(improv, pred_std + 1E-9)\n",
    "    acf = np.multiply(improv, norm.cdf(z_score)) + np.multiply(pred_std, norm.pdf(z_score))\n",
    "    return (-1.0) * acf * feasible_probs        # weighting acf value with Pr(feasible|X)\n",
    "\n",
    "#function to calculate class probabilities\n",
    "def class_probability(X, model):\n",
    "    with torch.no_grad():\n",
    "        logit_dist = model(torch.tensor(X, dtype=torch.float))\n",
    "    samples = logit_dist.sample(torch.Size((256,))).exp()\n",
    "    return (samples / samples.sum(-2, keepdim=True)).mean(0)\n",
    "\n",
    "# inverse transform \n",
    "def inverse_transform(val, mean, std):\n",
    "    return std * val + mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 - Best Acf value = -0.049\n",
      "Iteration 10 - Best Acf value = -0.049\n",
      "Iteration 20 - Best Acf value = -0.049\n",
      "Iteration 30 - Best Acf value = -0.049\n",
      "Iteration 40 - Best Acf value = -0.049\n"
     ]
    }
   ],
   "source": [
    "# search for best point\n",
    "n_restarts = 50\n",
    "# x_bounds = np.array([[2000, 10000], [0, 100], [0, 40], [5000, 15000], [80, 100], [0,100], [60, 100], [70, 100]])\n",
    "x_bounds = np.c_[x_mins, x_maxs]\n",
    "search_grid = np.random.uniform(x_bounds[:, 0], x_bounds[:, 1], size=(10*n_restarts, len(x_bounds)))\n",
    "\n",
    "mo_models = [reg_model1, reg_model2, reg_model3, cls_model] # includes all necessary models for optimization\n",
    "mo_likls = [reg_likl1, reg_likl2, reg_likl3, cls_likl]\n",
    "\n",
    "# Swith on eval mode\n",
    "for i in range(len(mo_models)):\n",
    "    mo_models[i].eval()\n",
    "    mo_likls[i].eval()\n",
    "\n",
    "y_best_curr = torch.max((train_y1 + train_y2 + train_y3)).item()\n",
    "\n",
    "acf_vals = [acquisition_func(search_grid[i, :].reshape(1, -1), mo_models, mo_likls, y_best_curr) for i in range(10*n_restarts)]\n",
    "\n",
    "acf_vals = np.array(acf_vals).reshape(10*n_restarts,)\n",
    "top_idx = np.argsort(acf_vals)\n",
    "search_grid = search_grid[top_idx[0:n_restarts]].reshape(n_restarts,-1)\n",
    "\n",
    "best_acquisition_values = 1\n",
    "best_x = None\n",
    "for i, starting_point in enumerate(search_grid):\n",
    "    res = minimize(fun=acquisition_func, \n",
    "                   x0=starting_point, \n",
    "                   bounds=x_bounds,\n",
    "                   method='L-BFGS-B',\n",
    "                   args=(mo_models, mo_likls, y_best_curr))\n",
    "    if res.fun < best_acquisition_values:\n",
    "        best_acquisition_values = res.fun\n",
    "        best_x = res.x\n",
    "    #print every 10 starting points\n",
    "    if i%10 == 0:\n",
    "        print('Iteration {0:d} - Best Acf value = {1:.3f}'.format(i, best_acquisition_values))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_point = best_x\n",
    "new_acf = (-1.0) * best_acquisition_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_output(X, models, likelihoods, transsform_func, scaling):\n",
    "    '''\n",
    "    X: (np array) new best point\n",
    "    models: (list) all model objects ; regression first and last model is classification\n",
    "    likelihoods: (list) likelihood objects; last one is classification\n",
    "    transform_func: (function handle) function to inverse transform all objective values\n",
    "    scaling: (dictionary) values of x and y means and standard deviation used for scaling\n",
    "    '''\n",
    "    X = torch.tensor(X.reshape(1,-1), dtype=torch.float)\n",
    "    obj_models = models[0:-1]\n",
    "    class_model = models[-1]\n",
    "    # value of objective function - inverse transformed\n",
    "    obj_likls = likelihoods[0:-1]\n",
    "    obj_preds = []\n",
    "    for i in range(len(obj_models)):\n",
    "        pred, _ = predict(obj_models[i], obj_likls[i], X)\n",
    "        obj_preds.append(transsform_func(pred, scaling['ymeans'].values[i], scaling['ystddv'].values[i])) \n",
    "    # feasibility\n",
    "    logit_dist = class_model(X)\n",
    "    samples = logit_dist.sample(torch.Size((256,))).exp()\n",
    "    class_probabilities = (samples / samples.sum(-2, keepdim=True)).mean(0)\n",
    "    feasibility = class_probabilities[1] # Pr(feasible|X)\n",
    "    #inverse transformed input\n",
    "    X_tr = [transsform_func(X[0][i].item(), scaling['xmeans'].values[i], scaling['xstddv'].values[i]) for i in range(8)]\n",
    "\n",
    "    return X_tr, obj_preds, feasibility\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_pt_tr, new_obj, new_feasib = new_output(new_point, mo_models, mo_likls, inverse_transform, scaling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5235.69939167,    65.04736558,    24.80448022, 13501.47653811,\n",
       "          91.62501695,    76.91168355,    63.28927553,    99.5972619 ])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(new_pt_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([171.7722]), tensor([35.4881]), tensor([11.7783])]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([11.7783])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predict shear modulus -\n",
    "reg_model3.eval()\n",
    "reg_likl3.eval()\n",
    "y3_pred, _ = predict(reg_model3, reg_likl3, torch.tensor(new_point.reshape(1,-1), dtype=torch.float))\n",
    "inverse_transform(y3_pred, ymeans.values[2], ystddv.values[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.9459])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_feasib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
