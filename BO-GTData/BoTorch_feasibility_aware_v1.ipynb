{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use BoTorch for feasibility weighted acquisition function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libearies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gouravkumbhojkar/miniconda3/envs/botorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import botorch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#modules for regression\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "\n",
    "#modules for BO\n",
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import (\n",
    "    FastNondominatedPartitioning,\n",
    ")\n",
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qExpectedHypervolumeImprovement,\n",
    "    qNoisyExpectedHypervolumeImprovement,\n",
    ")\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import (\n",
    "    DominatedPartitioning,\n",
    ")\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'../data/olhs_combine.xlsx'\n",
    "x_pd = pd.read_excel(filename, sheet_name='Design', header=[0,1], index_col=[0])\n",
    "y_pd = pd.read_excel(filename, sheet_name='bo_data', header=[0,1], index_col=[0])\n",
    "\n",
    "dtype=torch.double\n",
    "\n",
    "objective_properties = ['Polymer Solubility', 'Gelation Enthalpy', 'Shear Modulus']\n",
    "\n",
    "x = torch.tensor(x_pd.values, dtype=dtype)\n",
    "y = torch.tensor(y_pd[objective_properties].values, dtype=dtype)\n",
    "mfg = torch.tensor(y_pd['Manufacturability'].values, dtype=torch.long)\n",
    "\n",
    "x_bounds = np.array([[2000, 10000], [0, 100], [0, 40], [5000, 15000], [80, 100], [0,100], [60, 100], [70, 100]])\n",
    "x_bounds = torch.tensor(x_bounds.T, dtype=dtype)\n",
    "\n",
    "x = normalize(x, bounds=x_bounds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and initialize regression and classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model\n",
    "models = []\n",
    "for i in range(len(objective_properties)):\n",
    "    models.append(SingleTaskGP(x, y[:, i].unsqueeze(-1), \n",
    "                               outcome_transform=Standardize(m=1)))\n",
    "\n",
    "# transformation of mfg labels\n",
    "tmp_likl = DirichletClassificationLikelihood(targets=mfg.squeeze(), alpha_epsilon=1e-4, \n",
    "                                             learn_additional_noise=False)\n",
    "cls_model = SingleTaskGP(train_X=x, train_Y=-tmp_likl.transformed_targets.T.double(), \n",
    "                         outcome_transform=Standardize(m=2))\n",
    "\n",
    "models.append(cls_model)\n",
    "# mll_cls = ExactMarginalLogLikelihood(cls_model.likelihood, cls_model)\n",
    "\n",
    "model = ModelListGP(*models)\n",
    "mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "mll = mll.to(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the models to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SumMarginalLogLikelihood(\n",
       "  (likelihood): LikelihoodList(\n",
       "    (likelihoods): ModuleList(\n",
       "      (0-3): 4 x GaussianLikelihood(\n",
       "        (noise_covar): HomoskedasticNoise(\n",
       "          (noise_prior): GammaPrior()\n",
       "          (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): ModelListGP(\n",
       "    (models): ModuleList(\n",
       "      (0-3): 4 x SingleTaskGP(\n",
       "        (likelihood): GaussianLikelihood(\n",
       "          (noise_covar): HomoskedasticNoise(\n",
       "            (noise_prior): GammaPrior()\n",
       "            (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "          )\n",
       "        )\n",
       "        (mean_module): ConstantMean()\n",
       "        (covar_module): ScaleKernel(\n",
       "          (base_kernel): MaternKernel(\n",
       "            (lengthscale_prior): GammaPrior()\n",
       "            (raw_lengthscale_constraint): Positive()\n",
       "          )\n",
       "          (outputscale_prior): GammaPrior()\n",
       "          (raw_outputscale_constraint): Positive()\n",
       "        )\n",
       "        (outcome_transform): Standardize()\n",
       "      )\n",
       "    )\n",
       "    (likelihood): LikelihoodList(\n",
       "      (likelihoods): ModuleList(\n",
       "        (0-3): 4 x GaussianLikelihood(\n",
       "          (noise_covar): HomoskedasticNoise(\n",
       "            (noise_prior): GammaPrior()\n",
       "            (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlls): ModuleList(\n",
       "    (0-3): 4 x ExactMarginalLogLikelihood(\n",
       "      (likelihood): GaussianLikelihood(\n",
       "        (noise_covar): HomoskedasticNoise(\n",
       "          (noise_prior): GammaPrior()\n",
       "          (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "        )\n",
       "      )\n",
       "      (model): SingleTaskGP(\n",
       "        (likelihood): GaussianLikelihood(\n",
       "          (noise_covar): HomoskedasticNoise(\n",
       "            (noise_prior): GammaPrior()\n",
       "            (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "          )\n",
       "        )\n",
       "        (mean_module): ConstantMean()\n",
       "        (covar_module): ScaleKernel(\n",
       "          (base_kernel): MaternKernel(\n",
       "            (lengthscale_prior): GammaPrior()\n",
       "            (raw_lengthscale_constraint): Positive()\n",
       "          )\n",
       "          (outputscale_prior): GammaPrior()\n",
       "          (raw_outputscale_constraint): Positive()\n",
       "        )\n",
       "        (outcome_transform): Standardize()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regression model\n",
    "fit_gpytorch_mll(mll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.0338e+01, 3.4804e+00, 5.7093e+00, 1.3639e+01, 5.2273e-01],\n",
       "        [3.6136e+01, 5.2524e+01, 2.3242e+00, 1.1603e+00, 1.3002e+01],\n",
       "        [4.6839e+01, 5.4779e+00, 2.7008e+00, 1.2017e+00, 1.2960e+01],\n",
       "        [4.1833e+01, 4.7513e+00, 2.5480e+00, 1.0066e+00, 1.3155e+01],\n",
       "        [1.2645e+02, 2.3166e+00, 3.2931e+00, 1.3694e+01, 4.6791e-01],\n",
       "        [9.1096e+01, 8.0907e+01, 2.8848e+00, 1.3672e+01, 4.9006e-01],\n",
       "        [9.4297e+01, 2.7102e+00, 3.7552e+00, 1.3590e+01, 5.7191e-01],\n",
       "        [7.5021e+01, 2.6904e+01, 4.7946e+00, 1.3638e+01, 5.2376e-01],\n",
       "        [5.7483e+01, 1.0567e+02, 5.8902e+00, 1.3650e+01, 5.1176e-01],\n",
       "        [1.7562e+02, 3.9078e+00, 1.0532e+01, 1.3692e+01, 4.6987e-01],\n",
       "        [5.7746e+01, 1.1100e+02, 1.6696e+00, 1.3527e+01, 6.3490e-01],\n",
       "        [1.0777e+02, 1.7486e+01, 3.3631e+00, 1.3655e+01, 5.0665e-01],\n",
       "        [8.7002e+01, 9.2771e+00, 1.0524e+01, 1.3645e+01, 5.1683e-01],\n",
       "        [7.3058e+01, 1.0360e+02, 3.8432e+01, 1.3589e+01, 5.7306e-01],\n",
       "        [5.4783e+01, 4.2696e+01, 2.0682e+00, 1.1541e+00, 1.3008e+01],\n",
       "        [1.2251e+02, 3.9541e+00, 1.0110e+01, 1.3649e+01, 5.1324e-01],\n",
       "        [1.5140e+02, 1.6565e+01, 2.7813e+01, 1.3701e+01, 4.6078e-01],\n",
       "        [8.4396e+01, 2.8310e+01, 1.5320e+01, 1.2288e+00, 1.2933e+01],\n",
       "        [1.2016e+02, 2.9420e+00, 1.8671e+00, 1.3688e+01, 4.7447e-01],\n",
       "        [6.7001e+01, 3.8374e+00, 1.6663e+00, 1.1018e+00, 1.3060e+01],\n",
       "        [4.4956e+01, 3.7049e+00, 4.0566e+00, 1.1019e+00, 1.3060e+01],\n",
       "        [6.9934e+01, 2.8474e+01, 1.6972e+00, 1.3599e+01, 5.6288e-01],\n",
       "        [2.0493e+02, 5.1203e+00, 4.1353e+00, 1.3671e+01, 4.9085e-01],\n",
       "        [8.9426e+01, 1.3903e+01, 3.5040e+00, 1.3648e+01, 5.1388e-01],\n",
       "        [1.5092e+02, 5.2909e+00, 8.5322e+00, 1.3608e+01, 5.5399e-01],\n",
       "        [7.6421e+01, 4.3417e+00, 3.0353e+00, 1.3566e+01, 5.9608e-01],\n",
       "        [5.9623e+01, 5.9307e+01, 4.6543e+00, 1.3643e+01, 5.1932e-01],\n",
       "        [1.1919e+02, 4.1217e+01, 6.1605e+00, 1.3605e+01, 5.5719e-01],\n",
       "        [1.3033e+02, 4.1219e+01, 5.7022e+01, 1.3636e+01, 5.2557e-01],\n",
       "        [6.9270e+02, 2.5519e+01, 2.2688e+00, 1.3496e+01, 6.6565e-01],\n",
       "        [1.4261e+02, 6.5446e+00, 8.2969e+01, 1.3664e+01, 4.9779e-01],\n",
       "        [2.2938e+02, 3.0318e+00, 1.4345e+00, 1.3612e+01, 5.5006e-01],\n",
       "        [3.6375e+02, 6.0095e+00, 1.0760e+01, 1.3626e+01, 5.3602e-01],\n",
       "        [3.3181e+02, 3.4929e+00, 7.4631e+01, 1.0378e+00, 1.3124e+01],\n",
       "        [1.2417e+02, 4.3596e+00, 2.3508e+01, 1.3576e+01, 5.8583e-01],\n",
       "        [1.7664e+02, 3.7019e+00, 3.1583e+00, 1.3441e+01, 7.2122e-01],\n",
       "        [2.6104e+02, 3.4017e+00, 2.4494e+00, 1.3629e+01, 5.3327e-01],\n",
       "        [3.0758e+02, 8.4537e+00, 1.0384e+01, 1.3592e+01, 5.7003e-01],\n",
       "        [1.1703e+02, 5.3386e+00, 5.6342e+01, 1.3693e+01, 4.6869e-01],\n",
       "        [5.8793e+02, 3.0691e+00, 1.9540e+01, 1.3540e+01, 6.2155e-01]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    posterior = model.posterior(x)\n",
    "    pred_mean = posterior.mean\n",
    "pred_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9.9998e-01, 4.5079e-05, 6.1726e-05, 3.3730e-05, 9.9999e-01, 9.9997e-01,\n",
       "        9.9999e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01, 9.9998e-01, 9.9999e-01,\n",
       "        9.9998e-01, 9.9999e-01, 4.5224e-05, 9.9998e-01, 9.9999e-01, 3.7471e-05,\n",
       "        9.9999e-01, 5.9146e-05, 5.0092e-05, 9.9998e-01, 9.9998e-01, 9.9999e-01,\n",
       "        9.9998e-01, 9.9997e-01, 9.9998e-01, 9.9999e-01, 9.9999e-01, 9.9998e-01,\n",
       "        9.9999e-01, 9.9998e-01, 9.9999e-01, 3.5988e-05, 9.9999e-01, 9.9998e-01,\n",
       "        9.9998e-01, 9.9999e-01, 9.9999e-01, 9.9996e-01], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# cls_model.eval()\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     posterior = cls_model.posterior(x)\n",
    "# samples = posterior.sample(torch.Size((256,))).exp()\n",
    "# probabilities = (samples / samples.sum(-1, keepdim=True)).mean(0)\n",
    "# probabilities[:, 0] # 1st col gives probability of manufacturability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = torch.zeros(2, 8)\n",
    "bounds[1] = 1\n",
    "\n",
    "BATCH_SIZE = 4      # Number of candidates selected in each BO run/iteration\n",
    "NUM_RESTARTS = 10   # Restarts during BO run\n",
    "RAW_SAMPLES = 512   \n",
    "\n",
    "ref_point = ref_point = torch.tensor([18, 0.1, 0.01], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def obj_callable(Z: torch.Tensor, X: Optional[torch.Tensor] = None):\n",
    "    return IdentityMCMultiOutputObjective([0,1,2])\n",
    "\n",
    "def constraint_callable(Z):\n",
    "    class0 = Z[..., -2]\n",
    "    class1 = Z[..., -1]\n",
    "    return class1 - class0 #neg for feasiblle values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, x_bounds)).mean\n",
    "    \n",
    "    # partitioning = FastNondominatedPartitioning(\n",
    "    #     ref_point= ref_point,\n",
    "    #     Y=pred,\n",
    "    # )\n",
    "\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=ref_point,\n",
    "        X_baseline=train_x,\n",
    "        sampler=sampler,\n",
    "        prune_baseline=True,\n",
    "        objective=IdentityMCMultiOutputObjective(outcomes=[0, 1, 2]),\n",
    "        constraints=[constraint_callable],\n",
    "    )\n",
    "\n",
    "    #optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "        options={\"batch_limit\":5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "\n",
    "    new_x = unnormalize(candidates.detach(), bounds=x_bounds)\n",
    "    return new_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SobolQMCNormalSampler(sample_shape=torch.Size([128]))\n",
    "\n",
    "new_x_qnehvi = optimize_qnehvi_and_get_observation(model, x, y, sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.4049e+03, 6.7979e+01, 2.9767e+01, 1.5000e+04, 9.8777e+01, 3.4605e+00,\n",
       "         8.6779e+01, 8.7559e+01],\n",
       "        [4.5762e+03, 4.8585e+01, 2.2482e+01, 1.4433e+04, 9.1648e+01, 7.5514e+01,\n",
       "         9.2885e+01, 8.5352e+01],\n",
       "        [4.8585e+03, 4.5736e+01, 2.5860e+01, 1.4606e+04, 9.4307e+01, 8.1446e+01,\n",
       "         9.1930e+01, 9.1791e+01],\n",
       "        [3.8224e+03, 6.3609e+01, 1.8460e+01, 1.4202e+04, 8.7815e+01, 7.4481e+01,\n",
       "         9.4315e+01, 7.7646e+01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_qnehvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[148.9710,  22.8098,  13.7619,  11.1271,   3.0349],\n",
       "        [148.9710,  22.8098,  13.7619,  11.1271,   3.0349],\n",
       "        [148.9710,  22.8098,  13.7619,  11.1271,   3.0349],\n",
       "        [148.9710,  22.8098,  13.7619,  11.1271,   3.0349]],\n",
       "       dtype=torch.float64, grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.posterior(new_x_qnehvi).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1713, 0.8287],\n",
       "        [0.1863, 0.8137],\n",
       "        [0.1441, 0.8559],\n",
       "        [0.1521, 0.8479]], dtype=torch.float64)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_likl = DirichletClassificationLikelihood(targets=mfg.squeeze(), alpha_epsilon=1e-4, learn_additional_noise=False)\n",
    "cls_model = SingleTaskGP(train_X=x, train_Y=cls_likl.transformed_targets.T.double(), outcome_transform=Standardize(m=2))\n",
    "mll_cls = ExactMarginalLogLikelihood(cls_model.likelihood, cls_model)\n",
    "fit_gpytorch_mll(mll_cls)\n",
    "\n",
    "cls_model.eval()\n",
    "with torch.no_grad():\n",
    "    post_cls = cls_model.posterior(new_x_qnehvi)\n",
    "samples_cls = post_cls.sample(torch.Size((256,))).exp()\n",
    "probabilities = (samples_cls / samples_cls.sum(-1, keepdim=True)).mean(0)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
