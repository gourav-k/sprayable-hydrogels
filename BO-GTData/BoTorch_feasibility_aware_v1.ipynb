{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Use BoTorch for feasibility weighted acquisition function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import libearies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\goura\\anaconda3\\envs\\botorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import botorch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#modules for regression\n",
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import ExactMarginalLogLikelihood\n",
    "from botorch.utils.transforms import unnormalize, normalize\n",
    "\n",
    "#modules for BO\n",
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import (\n",
    "    FastNondominatedPartitioning,\n",
    ")\n",
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qExpectedHypervolumeImprovement,\n",
    "    qNoisyExpectedHypervolumeImprovement,\n",
    ")\n",
    "from botorch.utils.sampling import sample_simplex\n",
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import (\n",
    "    DominatedPartitioning,\n",
    ")\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated\n",
    "\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from botorch.acquisition.multi_objective.objective import IdentityMCMultiOutputObjective\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read and normalize data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'../data/olhs_combine.xlsx'\n",
    "x_pd = pd.read_excel(filename, sheet_name='Design', header=[0,1], index_col=[0])\n",
    "y_pd = pd.read_excel(filename, sheet_name='bo_data', header=[0,1], index_col=[0])\n",
    "\n",
    "dtype=torch.double\n",
    "\n",
    "objective_properties = ['Polymer Solubility', 'Gelation Enthalpy', 'Shear Modulus']\n",
    "\n",
    "x = torch.tensor(x_pd.values, dtype=dtype)\n",
    "y = torch.tensor(y_pd[objective_properties].values, dtype=dtype)\n",
    "mfg = torch.tensor(y_pd['Manufacturability'].values, dtype=torch.long)\n",
    "\n",
    "x_bounds = np.array([[2000, 10000], [0, 100], [0, 40], [5000, 15000], [80, 100], [0,100], [60, 100], [70, 100]])\n",
    "x_bounds = torch.tensor(x_bounds.T, dtype=dtype)\n",
    "\n",
    "x = normalize(x, bounds=x_bounds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify non-feasible designs\n",
    "feas_bool = mfg==1\n",
    "x_reg = x[feas_bool.flatten(), :] \n",
    "y_reg = y[feas_bool.flatten(), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and initialize regression and classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regression model\n",
    "models = []\n",
    "for i in range(len(objective_properties)):\n",
    "    models.append(SingleTaskGP(x_reg, y_reg[:, i].unsqueeze(-1), \n",
    "                               outcome_transform=Standardize(m=1)))\n",
    "\n",
    "# transformation of mfg labels\n",
    "tmp_likl = DirichletClassificationLikelihood(targets=mfg.squeeze(), alpha_epsilon=1e-4, \n",
    "                                             learn_additional_noise=False)\n",
    "cls_model = SingleTaskGP(train_X=x, train_Y=-tmp_likl.transformed_targets.T.double(), \n",
    "                         outcome_transform=Standardize(m=2))\n",
    "\n",
    "models.append(cls_model)\n",
    "\n",
    "model = ModelListGP(*models)\n",
    "mll = SumMarginalLogLikelihood(model.likelihood, model)\n",
    "mll = mll.to(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fit the models to data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SumMarginalLogLikelihood(\n",
       "  (likelihood): LikelihoodList(\n",
       "    (likelihoods): ModuleList(\n",
       "      (0-3): 4 x GaussianLikelihood(\n",
       "        (noise_covar): HomoskedasticNoise(\n",
       "          (noise_prior): GammaPrior()\n",
       "          (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (model): ModelListGP(\n",
       "    (models): ModuleList(\n",
       "      (0-3): 4 x SingleTaskGP(\n",
       "        (likelihood): GaussianLikelihood(\n",
       "          (noise_covar): HomoskedasticNoise(\n",
       "            (noise_prior): GammaPrior()\n",
       "            (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "          )\n",
       "        )\n",
       "        (mean_module): ConstantMean()\n",
       "        (covar_module): ScaleKernel(\n",
       "          (base_kernel): MaternKernel(\n",
       "            (lengthscale_prior): GammaPrior()\n",
       "            (raw_lengthscale_constraint): Positive()\n",
       "          )\n",
       "          (outputscale_prior): GammaPrior()\n",
       "          (raw_outputscale_constraint): Positive()\n",
       "        )\n",
       "        (outcome_transform): Standardize()\n",
       "      )\n",
       "    )\n",
       "    (likelihood): LikelihoodList(\n",
       "      (likelihoods): ModuleList(\n",
       "        (0-3): 4 x GaussianLikelihood(\n",
       "          (noise_covar): HomoskedasticNoise(\n",
       "            (noise_prior): GammaPrior()\n",
       "            (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (mlls): ModuleList(\n",
       "    (0-3): 4 x ExactMarginalLogLikelihood(\n",
       "      (likelihood): GaussianLikelihood(\n",
       "        (noise_covar): HomoskedasticNoise(\n",
       "          (noise_prior): GammaPrior()\n",
       "          (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "        )\n",
       "      )\n",
       "      (model): SingleTaskGP(\n",
       "        (likelihood): GaussianLikelihood(\n",
       "          (noise_covar): HomoskedasticNoise(\n",
       "            (noise_prior): GammaPrior()\n",
       "            (raw_noise_constraint): GreaterThan(1.000E-04)\n",
       "          )\n",
       "        )\n",
       "        (mean_module): ConstantMean()\n",
       "        (covar_module): ScaleKernel(\n",
       "          (base_kernel): MaternKernel(\n",
       "            (lengthscale_prior): GammaPrior()\n",
       "            (raw_lengthscale_constraint): Positive()\n",
       "          )\n",
       "          (outputscale_prior): GammaPrior()\n",
       "          (raw_outputscale_constraint): Positive()\n",
       "        )\n",
       "        (outcome_transform): Standardize()\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# regression model\n",
    "fit_gpytorch_mll(mll)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prediction for sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5.6105e+01, 3.2461e+00, 5.5199e+00, 1.3639e+01, 5.2274e-01],\n",
       "        [2.2391e+02, 2.1208e+01, 1.6374e+01, 1.1603e+00, 1.3002e+01],\n",
       "        [1.6535e+02, 2.1728e+01, 1.6213e+01, 1.2017e+00, 1.2960e+01],\n",
       "        [1.8709e+02, 2.4024e+01, 1.3192e+01, 1.0066e+00, 1.3155e+01],\n",
       "        [1.2611e+02, 2.0011e+00, 3.0697e+00, 1.3694e+01, 4.6791e-01],\n",
       "        [8.9072e+01, 8.2121e+01, 2.5984e+00, 1.3672e+01, 4.9007e-01],\n",
       "        [9.0486e+01, 2.4459e+00, 3.4244e+00, 1.3590e+01, 5.7192e-01],\n",
       "        [7.0479e+01, 2.7199e+01, 4.3013e+00, 1.3638e+01, 5.2377e-01],\n",
       "        [5.4015e+01, 1.0730e+02, 5.7114e+00, 1.3650e+01, 5.1176e-01],\n",
       "        [1.7547e+02, 3.7080e+00, 1.0451e+01, 1.3692e+01, 4.6988e-01],\n",
       "        [5.4411e+01, 1.1299e+02, 1.4331e+00, 1.3527e+01, 6.3491e-01],\n",
       "        [1.0590e+02, 1.7457e+01, 3.1507e+00, 1.3655e+01, 5.0666e-01],\n",
       "        [8.4093e+01, 9.1354e+00, 1.0484e+01, 1.3645e+01, 5.1683e-01],\n",
       "        [7.0150e+01, 1.0539e+02, 3.9058e+01, 1.3589e+01, 5.7307e-01],\n",
       "        [3.3197e+02, 2.2411e+01, 1.0728e+01, 1.1540e+00, 1.3008e+01],\n",
       "        [1.2207e+02, 3.7478e+00, 1.0049e+01, 1.3649e+01, 5.1324e-01],\n",
       "        [1.5188e+02, 1.6597e+01, 2.8162e+01, 1.3701e+01, 4.6078e-01],\n",
       "        [1.1928e+02, 2.2490e+01, 1.1059e+01, 1.2287e+00, 1.2933e+01],\n",
       "        [1.1856e+02, 2.6975e+00, 1.5107e+00, 1.3688e+01, 4.7448e-01],\n",
       "        [1.7985e+02, 2.1383e+01, 1.1343e+01, 1.1018e+00, 1.3060e+01],\n",
       "        [1.7607e+02, 3.4700e+01, 1.3907e+01, 1.1019e+00, 1.3060e+01],\n",
       "        [6.5980e+01, 2.8630e+01, 1.4611e+00, 1.3599e+01, 5.6289e-01],\n",
       "        [2.0867e+02, 4.8435e+00, 3.9341e+00, 1.3671e+01, 4.9086e-01],\n",
       "        [8.7024e+01, 1.3959e+01, 3.2630e+00, 1.3648e+01, 5.1388e-01],\n",
       "        [1.5215e+02, 4.9420e+00, 8.3510e+00, 1.3608e+01, 5.5399e-01],\n",
       "        [7.3597e+01, 4.0712e+00, 2.7697e+00, 1.3566e+01, 5.9608e-01],\n",
       "        [5.5260e+01, 6.0028e+01, 4.4657e+00, 1.3643e+01, 5.1933e-01],\n",
       "        [1.1841e+02, 4.1659e+01, 5.9524e+00, 1.3605e+01, 5.5719e-01],\n",
       "        [1.3050e+02, 4.1742e+01, 5.8037e+01, 1.3636e+01, 5.2558e-01],\n",
       "        [7.2723e+02, 2.5347e+01, 2.1696e+00, 1.3496e+01, 6.6566e-01],\n",
       "        [1.4330e+02, 6.2000e+00, 8.4606e+01, 1.3664e+01, 4.9779e-01],\n",
       "        [2.3599e+02, 2.9039e+00, 1.2539e+00, 1.3612e+01, 5.5007e-01],\n",
       "        [3.7760e+02, 5.9421e+00, 1.0756e+01, 1.3626e+01, 5.3603e-01],\n",
       "        [1.7480e+02, 2.3882e+01, 1.2457e+01, 1.0378e+00, 1.3124e+01],\n",
       "        [1.2419e+02, 4.2460e+00, 2.3830e+01, 1.3576e+01, 5.8584e-01],\n",
       "        [1.8146e+02, 3.4238e+00, 3.1345e+00, 1.3441e+01, 7.2123e-01],\n",
       "        [2.6892e+02, 3.2760e+00, 2.2523e+00, 1.3629e+01, 5.3328e-01],\n",
       "        [3.1825e+02, 8.4212e+00, 1.0372e+01, 1.3592e+01, 5.7003e-01],\n",
       "        [1.1542e+02, 5.0985e+00, 5.7265e+01, 1.3693e+01, 4.6870e-01],\n",
       "        [6.1515e+02, 2.6999e+00, 1.9713e+01, 1.3540e+01, 6.2155e-01]],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    posterior = model.posterior(x)\n",
    "    pred_mean = posterior.mean\n",
    "pred_mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper functions for BO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds = torch.zeros(2, 8)\n",
    "bounds[1] = 1\n",
    "\n",
    "BATCH_SIZE = 4      # Number of candidates selected in each BO run/iteration\n",
    "NUM_RESTARTS = 10   # Restarts during BO run\n",
    "RAW_SAMPLES = 512   \n",
    "\n",
    "ref_point = ref_point = torch.tensor([18, 0.1, 0.01], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "\n",
    "def obj_callable(Z: torch.Tensor, X: Optional[torch.Tensor] = None):\n",
    "    return IdentityMCMultiOutputObjective([0,1,2])\n",
    "\n",
    "def constraint_callable(Z):\n",
    "    class0 = Z[..., -2]\n",
    "    class1 = Z[..., -1]\n",
    "    return class1 - class0 #neg for feasiblle values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize_qnehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, x_bounds)).mean\n",
    "\n",
    "    acq_func = qNoisyExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=ref_point,\n",
    "        X_baseline=train_x,\n",
    "        sampler=sampler,\n",
    "        prune_baseline=True,\n",
    "        objective=IdentityMCMultiOutputObjective(outcomes=[0, 1, 2]),\n",
    "        constraints=[constraint_callable],\n",
    "    )\n",
    "\n",
    "    #optimize\n",
    "    candidates, _ = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,\n",
    "        options={\"batch_limit\":5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "\n",
    "    new_x = unnormalize(candidates.detach(), bounds=x_bounds)\n",
    "    return new_x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = SobolQMCNormalSampler(sample_shape=torch.Size([128]))\n",
    "\n",
    "new_x_qnehvi = optimize_qnehvi_and_get_observation(model, x, y, sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4.5400e+03, 6.6770e+01, 2.9999e+01, 1.4463e+04, 9.8290e+01, 8.4361e+00,\n",
       "         8.7870e+01, 8.7271e+01],\n",
       "        [4.6462e+03, 5.1599e+01, 2.2852e+01, 1.4436e+04, 9.2162e+01, 6.4709e+01,\n",
       "         9.3740e+01, 8.5816e+01],\n",
       "        [4.5897e+03, 6.1354e+01, 2.6945e+01, 1.4385e+04, 9.6520e+01, 3.1376e+01,\n",
       "         9.0703e+01, 8.6327e+01],\n",
       "        [5.0443e+03, 0.0000e+00, 2.4051e+01, 1.4235e+04, 8.7934e+01, 7.5644e+01,\n",
       "         9.2554e+01, 9.2086e+01]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_x_qnehvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[170.7780,  24.1824,  13.7389,  11.1271,   3.0349],\n",
       "        [170.7780,  24.1824,  13.7389,  11.1271,   3.0349],\n",
       "        [170.7780,  24.1824,  13.7389,  11.1271,   3.0349],\n",
       "        [170.7780,  24.1824,  13.7389,  11.1271,   3.0349]],\n",
       "       dtype=torch.float64, grad_fn=<CloneBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.posterior(new_x_qnehvi).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.1588, 0.8412],\n",
       "        [0.1546, 0.8454],\n",
       "        [0.1302, 0.8698],\n",
       "        [0.1497, 0.8503]], dtype=torch.float64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_likl = DirichletClassificationLikelihood(targets=mfg.squeeze(), alpha_epsilon=1e-4, learn_additional_noise=False)\n",
    "cls_model = SingleTaskGP(train_X=x, train_Y=cls_likl.transformed_targets.T.double(), outcome_transform=Standardize(m=2))\n",
    "mll_cls = ExactMarginalLogLikelihood(cls_model.likelihood, cls_model)\n",
    "fit_gpytorch_mll(mll_cls)\n",
    "\n",
    "cls_model.eval()\n",
    "with torch.no_grad():\n",
    "    post_cls = cls_model.posterior(new_x_qnehvi)\n",
    "samples_cls = post_cls.sample(torch.Size((256,))).exp()\n",
    "probabilities = (samples_cls / samples_cls.sum(-1, keepdim=True)).mean(0)\n",
    "probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
