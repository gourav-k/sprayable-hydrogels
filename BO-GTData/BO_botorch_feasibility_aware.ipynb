{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Multi-objective optimization with feasibility aware Expected Hyper-volume Improvement**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gouravkumbhojkar/miniconda3/envs/botorch/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import botorch\n",
    "import gpytorch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.models.gp_regression import SingleTaskGP\n",
    "from botorch.models.model_list_gp_regression import ModelListGP\n",
    "from botorch.models.transforms.outcome import Standardize\n",
    "from gpytorch.mlls.sum_marginal_log_likelihood import SumMarginalLogLikelihood\n",
    "from botorch.utils.transforms import unnormalize, normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch.optim.optimize import optimize_acqf, optimize_acqf_list\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.utils.multi_objective.scalarization import get_chebyshev_scalarization\n",
    "from botorch.utils.multi_objective.box_decompositions.non_dominated import (\n",
    "    FastNondominatedPartitioning,\n",
    ")\n",
    "from botorch.acquisition.multi_objective.monte_carlo import (\n",
    "    qExpectedHypervolumeImprovement,\n",
    "    qNoisyExpectedHypervolumeImprovement,\n",
    ")\n",
    "from botorch.utils.sampling import sample_simplex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from botorch import fit_gpytorch_mll\n",
    "from botorch.exceptions import BadInitialCandidatesWarning\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "from botorch.utils.multi_objective.box_decompositions.dominated import (\n",
    "    DominatedPartitioning,\n",
    ")\n",
    "from botorch.utils.multi_objective.pareto import is_non_dominated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = r'../data/olhs_run1.xlsx'\n",
    "x_pd = pd.read_excel(filename, sheet_name='Initial Design (OLHS)', header=[0,1], index_col=[0])\n",
    "y_pd = pd.read_excel(filename, sheet_name='bo_data', header=[0,1], index_col=[0])\n",
    "\n",
    "# no normalization yet\n",
    "\n",
    "dtype=torch.double\n",
    "\n",
    "# temporarily take out some data for validation\n",
    "validation_idx = [1,7,15]\n",
    "\n",
    "train_x_pd = x_pd.drop(validation_idx)\n",
    "train_y_pd = y_pd.drop(validation_idx)\n",
    "\n",
    "# which properties to read from labels\n",
    "objective_properties = ['Polymer Solubility', 'Gelation Enthalpy', 'Shear Modulus']\n",
    "\n",
    "# make torch tensors \n",
    "train_x = torch.tensor(train_x_pd.values, dtype=dtype)\n",
    "train_y_list = []\n",
    "for prop in objective_properties:\n",
    "    train_y_list.append(\n",
    "        torch.tensor(train_y_pd[prop].values, dtype=dtype)\n",
    "    )\n",
    "train_mfg_labels = torch.tensor(train_y_pd['Manufacturability'].values, dtype=torch.long).squeeze()\n",
    "\n",
    "bounds = np.array([[2000, 10000], [0, 100], [0, 40], [5000, 15000], [80, 100], [0,100], [60, 100], [70, 100]])\n",
    "bounds = torch.tensor(bounds.T, dtype=dtype)\n",
    "\n",
    "# Normalize input parameters\n",
    "train_x = normalize(train_x, bounds=bounds) # normalized to unit hypercube - all values betn 0 and 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define and initialize classification model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define classification model using gpytorch\n",
    "#class definition\n",
    "class DirichletGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
    "            gpytorch.kernels.RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# function to optimize parameters of the classification GP - \n",
    "def train_cls_gp(model, likelihood, train_x, training_iter):\n",
    "   # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize classification model \n",
    "cls_likl = gpytorch.likelihoods.DirichletClassificationLikelihood(train_mfg_labels, learn_additional_noise=False, alpha_epsilon=1e-4)\n",
    "cls_model = DirichletGPModel(train_x, cls_likl.transformed_targets.double(), cls_likl, num_classes=cls_likl.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define  and initalzie Regression models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = []\n",
    "for data in train_y_list:\n",
    "    models.append(\n",
    "        SingleTaskGP(train_x, data, outcome_transform=Standardize(m=1))\n",
    "    )\n",
    "model = ModelListGP(*models)\n",
    "mll = SumMarginalLogLikelihood(model.likelihood, model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define Acquisition function helper functions\n",
    "\n",
    "$$constrained \\hspace{3mm} acf = P * acf$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constraint_func(X):\n",
    "    #calculate probability of class 1 for X\n",
    "    cls_likl.eval()\n",
    "    cls_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit_dist = cls_model(X)\n",
    "    samples = logit_dist.sample(torch.Size((256,))).exp()\n",
    "    class_probs = (samples / samples.sum(-2, keepdim=True)).mean(0)\n",
    "    feasible_class_probs = class_probs[1]\n",
    "\n",
    "    threshold = 0.7\n",
    "    feasibility_condition = threshold - feasible_class_probs    #negative value implies feasibility\n",
    "    \n",
    "    return feasibility_condition.view(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following the botorch tutorial\n",
    "BATCH_SIZE = 4      # Number of candidates selected in each BO run/iteration\n",
    "NUM_RESTARTS = 10   # Restarts during BO run\n",
    "RAW_SAMPLES = 512   \n",
    "\n",
    "# function to optimize acquisition function\n",
    "standard_bounds = torch.zeros(2, 8)\n",
    "standard_bounds[1] = 1\n",
    "\n",
    "ref_point = torch.tensor([18, 0.1, 0.01], dtype=dtype)\n",
    "\n",
    "def optimize_qehvi_and_get_observation(model, train_x, train_obj, sampler):\n",
    "    \"\"\"Optimizes the qEHVI acquisition function, and returns a new candidate and observation.\"\"\"\n",
    "    # partition non-dominated space into disjoint rectangles\n",
    "    with torch.no_grad():\n",
    "        pred = model.posterior(normalize(train_x, bounds)).mean\n",
    "    \n",
    "    partitioning = FastNondominatedPartitioning(\n",
    "        ref_point= ref_point,\n",
    "        Y=pred,\n",
    "    )\n",
    "\n",
    "    acq_func = qExpectedHypervolumeImprovement(\n",
    "        model=model,\n",
    "        ref_point=ref_point,\n",
    "        partitioning=partitioning,\n",
    "        sampler=sampler,\n",
    "        constraints=[constraint_func],\n",
    "    )\n",
    "\n",
    "    # optimize\n",
    "    candidates, acq_vals = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=standard_bounds,\n",
    "        q=BATCH_SIZE,\n",
    "        num_restarts=NUM_RESTARTS,\n",
    "        raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "        options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "        sequential=True,\n",
    "    )\n",
    "    \n",
    "    # observe new values\n",
    "    new_x = unnormalize(candidates.detach(), bounds=bounds)\n",
    "\n",
    "    return new_x, acq_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building the BO framework"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "hvs_qehvi = []\n",
    "\n",
    "# fit regression models\n",
    "fit_gpytorch_mll(mll)\n",
    "\n",
    "#fit classification model\n",
    "cls_model, cls_likl = train_cls_gp(cls_model, cls_likl, train_x, training_iter=50)\n",
    "\n",
    "# define acquisition modules\n",
    "sampler = SobolQMCNormalSampler(sample_shape=torch.Size([128]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 8 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_x_qehvi, new_acq_vals \u001b[38;5;241m=\u001b[39m optimize_qehvi_and_get_observation(model, train_x, train_y_list, sampler)\n",
      "Cell \u001b[0;32mIn[20], line 32\u001b[0m, in \u001b[0;36moptimize_qehvi_and_get_observation\u001b[0;34m(model, train_x, train_obj, sampler)\u001b[0m\n\u001b[1;32m     23\u001b[0m acq_func \u001b[38;5;241m=\u001b[39m qExpectedHypervolumeImprovement(\n\u001b[1;32m     24\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[1;32m     25\u001b[0m     ref_point\u001b[38;5;241m=\u001b[39mref_point,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     28\u001b[0m     constraints\u001b[38;5;241m=\u001b[39m[constraint_func],\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     31\u001b[0m \u001b[38;5;66;03m# optimize\u001b[39;00m\n\u001b[0;32m---> 32\u001b[0m candidates, acq_vals \u001b[38;5;241m=\u001b[39m optimize_acqf(\n\u001b[1;32m     33\u001b[0m     acq_function\u001b[38;5;241m=\u001b[39macq_func,\n\u001b[1;32m     34\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mstandard_bounds,\n\u001b[1;32m     35\u001b[0m     q\u001b[38;5;241m=\u001b[39mBATCH_SIZE,\n\u001b[1;32m     36\u001b[0m     num_restarts\u001b[38;5;241m=\u001b[39mNUM_RESTARTS,\n\u001b[1;32m     37\u001b[0m     raw_samples\u001b[38;5;241m=\u001b[39mRAW_SAMPLES,  \u001b[38;5;66;03m# used for intialization heuristic\u001b[39;00m\n\u001b[1;32m     38\u001b[0m     options\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m5\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmaxiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m200\u001b[39m},\n\u001b[1;32m     39\u001b[0m     sequential\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     40\u001b[0m )\n\u001b[1;32m     42\u001b[0m \u001b[38;5;66;03m# observe new values\u001b[39;00m\n\u001b[1;32m     43\u001b[0m new_x \u001b[38;5;241m=\u001b[39m unnormalize(candidates\u001b[38;5;241m.\u001b[39mdetach(), bounds\u001b[38;5;241m=\u001b[39mbounds)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/optim/optimize.py:567\u001b[0m, in \u001b[0;36moptimize_acqf\u001b[0;34m(acq_function, bounds, q, num_restarts, raw_samples, options, inequality_constraints, equality_constraints, nonlinear_inequality_constraints, fixed_features, post_processing_func, batch_initial_conditions, return_best_only, gen_candidates, sequential, ic_generator, timeout_sec, return_full_tree, retry_on_optimization_warning, **ic_gen_kwargs)\u001b[0m\n\u001b[1;32m    544\u001b[0m     gen_candidates \u001b[38;5;241m=\u001b[39m gen_candidates_scipy\n\u001b[1;32m    545\u001b[0m opt_acqf_inputs \u001b[38;5;241m=\u001b[39m OptimizeAcqfInputs(\n\u001b[1;32m    546\u001b[0m     acq_function\u001b[38;5;241m=\u001b[39macq_function,\n\u001b[1;32m    547\u001b[0m     bounds\u001b[38;5;241m=\u001b[39mbounds,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m     ic_gen_kwargs\u001b[38;5;241m=\u001b[39mic_gen_kwargs,\n\u001b[1;32m    566\u001b[0m )\n\u001b[0;32m--> 567\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _optimize_acqf(opt_acqf_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/optim/optimize.py:585\u001b[0m, in \u001b[0;36m_optimize_acqf\u001b[0;34m(opt_inputs)\u001b[0m\n\u001b[1;32m    583\u001b[0m \u001b[38;5;66;03m# Perform sequential optimization via successive conditioning on pending points\u001b[39;00m\n\u001b[1;32m    584\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m opt_inputs\u001b[38;5;241m.\u001b[39msequential \u001b[38;5;129;01mand\u001b[39;00m opt_inputs\u001b[38;5;241m.\u001b[39mq \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m--> 585\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _optimize_acqf_sequential_q(opt_inputs\u001b[38;5;241m=\u001b[39mopt_inputs)\n\u001b[1;32m    587\u001b[0m \u001b[38;5;66;03m# Batch optimization (including the case q=1)\u001b[39;00m\n\u001b[1;32m    588\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _optimize_acqf_batch(opt_inputs\u001b[38;5;241m=\u001b[39mopt_inputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/optim/optimize.py:251\u001b[0m, in \u001b[0;36m_optimize_acqf_sequential_q\u001b[0;34m(opt_inputs)\u001b[0m\n\u001b[1;32m    242\u001b[0m new_inputs \u001b[38;5;241m=\u001b[39m dataclasses\u001b[38;5;241m.\u001b[39mreplace(\n\u001b[1;32m    243\u001b[0m     opt_inputs,\n\u001b[1;32m    244\u001b[0m     q\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    248\u001b[0m     timeout_sec\u001b[38;5;241m=\u001b[39mtimeout_sec,\n\u001b[1;32m    249\u001b[0m )\n\u001b[1;32m    250\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(opt_inputs\u001b[38;5;241m.\u001b[39mq):\n\u001b[0;32m--> 251\u001b[0m     candidate, acq_value \u001b[38;5;241m=\u001b[39m _optimize_acqf_batch(new_inputs)\n\u001b[1;32m    253\u001b[0m     candidate_list\u001b[38;5;241m.\u001b[39mappend(candidate)\n\u001b[1;32m    254\u001b[0m     acq_value_list\u001b[38;5;241m.\u001b[39mappend(acq_value)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/optim/optimize.py:275\u001b[0m, in \u001b[0;36m_optimize_acqf_batch\u001b[0;34m(opt_inputs)\u001b[0m\n\u001b[1;32m    272\u001b[0m     batch_initial_conditions \u001b[38;5;241m=\u001b[39m opt_inputs\u001b[38;5;241m.\u001b[39mbatch_initial_conditions\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    274\u001b[0m     \u001b[38;5;66;03m# pyre-ignore[28]: Unexpected keyword argument `acq_function` to anonymous call.\u001b[39;00m\n\u001b[0;32m--> 275\u001b[0m     batch_initial_conditions \u001b[38;5;241m=\u001b[39m opt_inputs\u001b[38;5;241m.\u001b[39mget_ic_generator()(\n\u001b[1;32m    276\u001b[0m         acq_function\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39macq_function,\n\u001b[1;32m    277\u001b[0m         bounds\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39mbounds,\n\u001b[1;32m    278\u001b[0m         q\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39mq,\n\u001b[1;32m    279\u001b[0m         num_restarts\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39mnum_restarts,\n\u001b[1;32m    280\u001b[0m         raw_samples\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39mraw_samples,\n\u001b[1;32m    281\u001b[0m         fixed_features\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39mfixed_features,\n\u001b[1;32m    282\u001b[0m         options\u001b[38;5;241m=\u001b[39moptions,\n\u001b[1;32m    283\u001b[0m         inequality_constraints\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39minequality_constraints,\n\u001b[1;32m    284\u001b[0m         equality_constraints\u001b[38;5;241m=\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39mequality_constraints,\n\u001b[1;32m    285\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mopt_inputs\u001b[38;5;241m.\u001b[39mic_gen_kwargs,\n\u001b[1;32m    286\u001b[0m     )\n\u001b[1;32m    288\u001b[0m batch_limit: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m options\u001b[38;5;241m.\u001b[39mget(\n\u001b[1;32m    289\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbatch_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    290\u001b[0m     (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m     ),\n\u001b[1;32m    295\u001b[0m )\n\u001b[1;32m    297\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_optimize_batch_candidates\u001b[39m() \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Tensor, List[\u001b[38;5;167;01mWarning\u001b[39;00m]]:\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/optim/initializers.py:417\u001b[0m, in \u001b[0;36mgen_batch_initial_conditions\u001b[0;34m(acq_function, bounds, q, num_restarts, raw_samples, fixed_features, options, inequality_constraints, equality_constraints, generator, fixed_X_fantasies)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m X_rnd\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    416\u001b[0m     end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_idx \u001b[38;5;241m+\u001b[39m batch_limit, X_rnd\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 417\u001b[0m     Y_rnd_curr \u001b[38;5;241m=\u001b[39m acq_function(\n\u001b[1;32m    418\u001b[0m         X_rnd[start_idx:end_idx]\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    419\u001b[0m     )\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    420\u001b[0m     Y_rnd_list\u001b[38;5;241m.\u001b[39mappend(Y_rnd_curr)\n\u001b[1;32m    421\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_limit\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/utils/transforms.py:305\u001b[0m, in \u001b[0;36mconcatenate_pending_points.<locals>.decorated\u001b[0;34m(cls, X, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mX_pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([X, match_batch_shape(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mX_pending, X)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mcls\u001b[39m, X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/utils/transforms.py:259\u001b[0m, in \u001b[0;36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[0;34m(acqf, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# add t-batch dim\u001b[39;00m\n\u001b[1;32m    258\u001b[0m X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 259\u001b[0m output \u001b[38;5;241m=\u001b[39m method(acqf, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(acqf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_ensemble(acqf\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# IDEA: this could be wrapped into SampleReducingMCAcquisitionFunction\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    263\u001b[0m         output\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m acqf\u001b[38;5;241m.\u001b[39m_log \u001b[38;5;28;01melse\u001b[39;00m logmeanexp(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    264\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/acquisition/multi_objective/monte_carlo.py:320\u001b[0m, in \u001b[0;36mqExpectedHypervolumeImprovement.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    318\u001b[0m posterior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mposterior(X)\n\u001b[1;32m    319\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_posterior_samples(posterior)\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_qehvi(samples\u001b[38;5;241m=\u001b[39msamples, X\u001b[38;5;241m=\u001b[39mX)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/acquisition/multi_objective/monte_carlo.py:244\u001b[0m, in \u001b[0;36mqExpectedHypervolumeImprovement._compute_qehvi\u001b[0;34m(self, samples, X)\u001b[0m\n\u001b[1;32m    242\u001b[0m q \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m     feas_weights \u001b[38;5;241m=\u001b[39m compute_smoothed_feasibility_indicator(\n\u001b[1;32m    245\u001b[0m         constraints\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints,\n\u001b[1;32m    246\u001b[0m         samples\u001b[38;5;241m=\u001b[39msamples,\n\u001b[1;32m    247\u001b[0m         eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta,\n\u001b[1;32m    248\u001b[0m         fat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfat,\n\u001b[1;32m    249\u001b[0m     )  \u001b[38;5;66;03m# `sample_shape x batch-shape x q`\u001b[39;00m\n\u001b[1;32m    250\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_point\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    251\u001b[0m q_subset_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_q_subset_indices(q_out\u001b[38;5;241m=\u001b[39mq, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/utils/objective.py:178\u001b[0m, in \u001b[0;36mcompute_smoothed_feasibility_indicator\u001b[0;34m(constraints, samples, eta, log, fat)\u001b[0m\n\u001b[1;32m    176\u001b[0m log_sigmoid \u001b[38;5;241m=\u001b[39m log_fatmoid \u001b[38;5;28;01mif\u001b[39;00m fat \u001b[38;5;28;01melse\u001b[39;00m logexpit\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m constraint, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(constraints, eta):\n\u001b[0;32m--> 178\u001b[0m     is_feasible \u001b[38;5;241m=\u001b[39m is_feasible \u001b[38;5;241m+\u001b[39m log_sigmoid(\u001b[38;5;241m-\u001b[39mconstraint(samples) \u001b[38;5;241m/\u001b[39m e)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_feasible \u001b[38;5;28;01mif\u001b[39;00m log \u001b[38;5;28;01melse\u001b[39;00m is_feasible\u001b[38;5;241m.\u001b[39mexp()\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mconstraint_func\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      4\u001b[0m cls_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     logit_dist \u001b[38;5;241m=\u001b[39m cls_model(X)\n\u001b[1;32m      7\u001b[0m samples \u001b[38;5;241m=\u001b[39m logit_dist\u001b[38;5;241m.\u001b[39msample(torch\u001b[38;5;241m.\u001b[39mSize((\u001b[38;5;241m256\u001b[39m,)))\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m      8\u001b[0m class_probs \u001b[38;5;241m=\u001b[39m (samples \u001b[38;5;241m/\u001b[39m samples\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:313\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m         train_input \u001b[38;5;241m=\u001b[39m train_input\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtrain_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m--> 313\u001b[0m     full_inputs\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mcat([train_input, \u001b[38;5;28minput\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Get the joint distribution for training/test data\u001b[39;00m\n\u001b[1;32m    316\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(ExactGP, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39mfull_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 8 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "new_x_qehvi, new_acq_vals = optimize_qehvi_and_get_observation(model, train_x, train_y_list, sampler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "need to change the optimize_qehvi function?\n",
    "\n",
    "Add constraint function to qEHVI definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# following the botorch tutorial\n",
    "BATCH_SIZE = 4      # Number of candidates selected in each BO run/iteration\n",
    "NUM_RESTARTS = 10   # Restarts during BO run\n",
    "RAW_SAMPLES = 512   \n",
    "\n",
    "# function to optimize acquisition function\n",
    "standard_bounds = torch.zeros(2, 8)\n",
    "standard_bounds[1] = 1\n",
    "\n",
    "ref_point = torch.tensor([18, 0.1, 0.01], dtype=dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition non-dominated space into disjoint rectangles\n",
    "with torch.no_grad():\n",
    "    pred = model.posterior(normalize(train_x, bounds)).mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioning = FastNondominatedPartitioning(\n",
    "    ref_point= ref_point,\n",
    "    Y=pred,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "acq_func = qExpectedHypervolumeImprovement(\n",
    "    model=model,\n",
    "    ref_point=ref_point,\n",
    "    partitioning=partitioning,\n",
    "    sampler=sampler,\n",
    "    constraints=[constraint_func],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Sizes of tensors must match except in dimension 2. Expected size 8 but got size 3 for tensor number 1 in the list.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[48], line 18\u001b[0m\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feasibility_condition\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mbotorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01moptim\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01minitializers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m gen_batch_initial_conditions\n\u001b[0;32m---> 18\u001b[0m Xinit \u001b[38;5;241m=\u001b[39m gen_batch_initial_conditions(acq_func, bounds, q\u001b[38;5;241m=\u001b[39mBATCH_SIZE, num_restarts\u001b[38;5;241m=\u001b[39mNUM_RESTARTS, raw_samples\u001b[38;5;241m=\u001b[39mRAW_SAMPLES)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/optim/initializers.py:417\u001b[0m, in \u001b[0;36mgen_batch_initial_conditions\u001b[0;34m(acq_function, bounds, q, num_restarts, raw_samples, fixed_features, options, inequality_constraints, equality_constraints, generator, fixed_X_fantasies)\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m start_idx \u001b[38;5;241m<\u001b[39m X_rnd\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[1;32m    416\u001b[0m     end_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(start_idx \u001b[38;5;241m+\u001b[39m batch_limit, X_rnd\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m--> 417\u001b[0m     Y_rnd_curr \u001b[38;5;241m=\u001b[39m acq_function(\n\u001b[1;32m    418\u001b[0m         X_rnd[start_idx:end_idx]\u001b[38;5;241m.\u001b[39mto(device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[1;32m    419\u001b[0m     )\u001b[38;5;241m.\u001b[39mcpu()\n\u001b[1;32m    420\u001b[0m     Y_rnd_list\u001b[38;5;241m.\u001b[39mappend(Y_rnd_curr)\n\u001b[1;32m    421\u001b[0m     start_idx \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m batch_limit\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/utils/transforms.py:305\u001b[0m, in \u001b[0;36mconcatenate_pending_points.<locals>.decorated\u001b[0;34m(cls, X, **kwargs)\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mX_pending \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    304\u001b[0m     X \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([X, match_batch_shape(\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39mX_pending, X)], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m method(\u001b[38;5;28mcls\u001b[39m, X, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/utils/transforms.py:259\u001b[0m, in \u001b[0;36mt_batch_mode_transform.<locals>.decorator.<locals>.decorated\u001b[0;34m(acqf, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[38;5;66;03m# add t-batch dim\u001b[39;00m\n\u001b[1;32m    258\u001b[0m X \u001b[38;5;241m=\u001b[39m X \u001b[38;5;28;01mif\u001b[39;00m X\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m X\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m--> 259\u001b[0m output \u001b[38;5;241m=\u001b[39m method(acqf, X, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    260\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(acqf, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m is_ensemble(acqf\u001b[38;5;241m.\u001b[39mmodel):\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;66;03m# IDEA: this could be wrapped into SampleReducingMCAcquisitionFunction\u001b[39;00m\n\u001b[1;32m    262\u001b[0m     output \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    263\u001b[0m         output\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m acqf\u001b[38;5;241m.\u001b[39m_log \u001b[38;5;28;01melse\u001b[39;00m logmeanexp(output, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    264\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/acquisition/multi_objective/monte_carlo.py:320\u001b[0m, in \u001b[0;36mqExpectedHypervolumeImprovement.forward\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    318\u001b[0m posterior \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mposterior(X)\n\u001b[1;32m    319\u001b[0m samples \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_posterior_samples(posterior)\n\u001b[0;32m--> 320\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_qehvi(samples\u001b[38;5;241m=\u001b[39msamples, X\u001b[38;5;241m=\u001b[39mX)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/acquisition/multi_objective/monte_carlo.py:244\u001b[0m, in \u001b[0;36mqExpectedHypervolumeImprovement._compute_qehvi\u001b[0;34m(self, samples, X)\u001b[0m\n\u001b[1;32m    242\u001b[0m q \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 244\u001b[0m     feas_weights \u001b[38;5;241m=\u001b[39m compute_smoothed_feasibility_indicator(\n\u001b[1;32m    245\u001b[0m         constraints\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconstraints,\n\u001b[1;32m    246\u001b[0m         samples\u001b[38;5;241m=\u001b[39msamples,\n\u001b[1;32m    247\u001b[0m         eta\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39meta,\n\u001b[1;32m    248\u001b[0m         fat\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfat,\n\u001b[1;32m    249\u001b[0m     )  \u001b[38;5;66;03m# `sample_shape x batch-shape x q`\u001b[39;00m\n\u001b[1;32m    250\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mref_point\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    251\u001b[0m q_subset_indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_q_subset_indices(q_out\u001b[38;5;241m=\u001b[39mq, device\u001b[38;5;241m=\u001b[39mdevice)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/botorch/utils/objective.py:178\u001b[0m, in \u001b[0;36mcompute_smoothed_feasibility_indicator\u001b[0;34m(constraints, samples, eta, log, fat)\u001b[0m\n\u001b[1;32m    176\u001b[0m log_sigmoid \u001b[38;5;241m=\u001b[39m log_fatmoid \u001b[38;5;28;01mif\u001b[39;00m fat \u001b[38;5;28;01melse\u001b[39;00m logexpit\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m constraint, e \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(constraints, eta):\n\u001b[0;32m--> 178\u001b[0m     is_feasible \u001b[38;5;241m=\u001b[39m is_feasible \u001b[38;5;241m+\u001b[39m log_sigmoid(\u001b[38;5;241m-\u001b[39mconstraint(samples) \u001b[38;5;241m/\u001b[39m e)\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m is_feasible \u001b[38;5;28;01mif\u001b[39;00m log \u001b[38;5;28;01melse\u001b[39;00m is_feasible\u001b[38;5;241m.\u001b[39mexp()\n",
      "Cell \u001b[0;32mIn[10], line 6\u001b[0m, in \u001b[0;36mconstraint_func\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      4\u001b[0m cls_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 6\u001b[0m     logit_dist \u001b[38;5;241m=\u001b[39m cls_model(X)\n\u001b[1;32m      7\u001b[0m samples \u001b[38;5;241m=\u001b[39m logit_dist\u001b[38;5;241m.\u001b[39msample(torch\u001b[38;5;241m.\u001b[39mSize((\u001b[38;5;241m256\u001b[39m,)))\u001b[38;5;241m.\u001b[39mexp()\n\u001b[1;32m      8\u001b[0m class_probs \u001b[38;5;241m=\u001b[39m (samples \u001b[38;5;241m/\u001b[39m samples\u001b[38;5;241m.\u001b[39msum(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m))\u001b[38;5;241m.\u001b[39mmean(\u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/botorch/lib/python3.11/site-packages/gpytorch/models/exact_gp.py:313\u001b[0m, in \u001b[0;36mExactGP.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    311\u001b[0m         train_input \u001b[38;5;241m=\u001b[39m train_input\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39mtrain_input\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[1;32m    312\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m*\u001b[39mbatch_shape, \u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:])\n\u001b[0;32m--> 313\u001b[0m     full_inputs\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39mcat([train_input, \u001b[38;5;28minput\u001b[39m], dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m))\n\u001b[1;32m    315\u001b[0m \u001b[38;5;66;03m# Get the joint distribution for training/test data\u001b[39;00m\n\u001b[1;32m    316\u001b[0m full_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msuper\u001b[39m(ExactGP, \u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39mfull_inputs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Sizes of tensors must match except in dimension 2. Expected size 8 but got size 3 for tensor number 1 in the list."
     ]
    }
   ],
   "source": [
    "def constraint_func(X):\n",
    "    #calculate probability of class 1 for X\n",
    "    cls_likl.eval()\n",
    "    cls_model.eval()\n",
    "    with torch.no_grad():\n",
    "        logit_dist = cls_model(X)\n",
    "    samples = logit_dist.sample(torch.Size((256,))).exp()\n",
    "    class_probs = (samples / samples.sum(-2, keepdim=True)).mean(0)\n",
    "    feasible_class_probs = class_probs[1]\n",
    "\n",
    "    threshold = 0.7\n",
    "    feasibility_condition = threshold - feasible_class_probs    #negative value implies feasibility\n",
    "    \n",
    "    return feasibility_condition.view(-1,1)\n",
    "\n",
    "\n",
    "from botorch.optim.initializers import gen_batch_initial_conditions\n",
    "Xinit = gen_batch_initial_conditions(acq_func, bounds, q=BATCH_SIZE, num_restarts=NUM_RESTARTS, raw_samples=RAW_SAMPLES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# optimize\n",
    "candidates, acq_vals = optimize_acqf(\n",
    "    acq_function=acq_func,\n",
    "    bounds=standard_bounds,\n",
    "    q=BATCH_SIZE,\n",
    "    num_restarts=NUM_RESTARTS,\n",
    "    raw_samples=RAW_SAMPLES,  # used for intialization heuristic\n",
    "    options={\"batch_limit\": 5, \"maxiter\": 200},\n",
    "    sequential=True,\n",
    ")\n",
    "\n",
    "# observe new values\n",
    "new_x = unnormalize(candidates.detach(), bounds=bounds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 23\u001b[0m\n\u001b[1;32m     19\u001b[0m         feasibility_condition \u001b[38;5;241m=\u001b[39m feasibility_condition\u001b[38;5;241m.\u001b[39mview(s, B, q, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m feasibility_condition\n\u001b[0;32m---> 23\u001b[0m constraint_func(train_x)\n",
      "Cell \u001b[0;32mIn[42], line 2\u001b[0m, in \u001b[0;36mconstraint_func\u001b[0;34m(X)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconstraint_func\u001b[39m(X):\n\u001b[0;32m----> 2\u001b[0m     s, B, q, d \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape  \u001b[38;5;66;03m# Extract the dimensions\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     cls_likl\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      4\u001b[0m     cls_model\u001b[38;5;241m.\u001b[39meval()\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "def constraint_func(X):\n",
    "    s, B, q, d = X.shape  # Extract the dimensions\n",
    "    cls_likl.eval()\n",
    "    cls_model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Reshape to (s * B * q, d) to pass through the model\n",
    "        X_flat = X.view(-1, d)\n",
    "        logit_dist = cls_model(X_flat)\n",
    "        \n",
    "        # Sample and compute class probabilities\n",
    "        samples = logit_dist.sample(torch.Size((256,))).exp()\n",
    "        class_probs = (samples / samples.sum(-2, keepdim=True)).mean(0)\n",
    "        feasible_class_probs = class_probs[1]\n",
    "        \n",
    "        threshold = 0.7\n",
    "        feasibility_condition = threshold - feasible_class_probs  # Negative value implies feasibility\n",
    "        \n",
    "        # Reshape the feasibility condition to (s, B, q, 1)\n",
    "        feasibility_condition = feasibility_condition.view(s, B, q, 1)\n",
    "    \n",
    "    return feasibility_condition\n",
    "\n",
    "constraint_func(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "botorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
