{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "import gpytorch\n",
    "from gpytorch.models import ExactGP\n",
    "from gpytorch.likelihoods import DirichletClassificationLikelihood\n",
    "from gpytorch.means import ConstantMean\n",
    "from gpytorch.kernels import ScaleKernel, RBFKernel\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GP class for regression\n",
    "class ExactGPModel(gpytorch.models.ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood):\n",
    "        super(ExactGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = gpytorch.means.ConstantMean()\n",
    "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# GP class for classification\n",
    "class DirichletGPModel(ExactGP):\n",
    "    def __init__(self, train_x, train_y, likelihood, num_classes):\n",
    "        super(DirichletGPModel, self).__init__(train_x, train_y, likelihood)\n",
    "        self.mean_module = ConstantMean(batch_shape=torch.Size((num_classes,)))\n",
    "        self.covar_module = ScaleKernel(\n",
    "            RBFKernel(batch_shape=torch.Size((num_classes,))),\n",
    "            batch_shape=torch.Size((num_classes,)),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        mean_x = self.mean_module(x)\n",
    "        covar_x = self.covar_module(x)\n",
    "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)\n",
    "\n",
    "# function to optimize parameters of the classification GP - \n",
    "def train_cls_gp(model, likelihood, train_x, training_iter):\n",
    "   # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, likelihood.transformed_targets).sum()\n",
    "        loss.backward()\n",
    "        if i + 1 == training_iter:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                model.covar_module.base_kernel.lengthscale.mean().item(),\n",
    "                model.likelihood.second_noise_covar.noise.mean().item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood\n",
    "\n",
    "# function to optimize parameters of the regression GP -\n",
    "def train_reg_gp(model, likelihood, train_x, train_y, training_iter):\n",
    "   # Find optimal model hyperparameters\n",
    "    model.train()\n",
    "    likelihood.train()\n",
    "\n",
    "    # Use the adam optimizer\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.1)  # Includes GaussianLikelihood parameters\n",
    "\n",
    "    # \"Loss\" for GPs - the marginal log likelihood\n",
    "    mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
    "\n",
    "    for i in range(training_iter):\n",
    "        # Zero gradients from previous iteration\n",
    "        optimizer.zero_grad()\n",
    "        # Output from model\n",
    "        output = model(train_x)\n",
    "        # Calc loss and backprop gradients\n",
    "        loss = -mll(output, train_y)\n",
    "        loss.backward()\n",
    "        if i + 1  == training_iter:\n",
    "            print('Iter %d/%d - Loss: %.3f   lengthscale: %.3f   noise: %.3f' % (\n",
    "                i + 1, training_iter, loss.item(),\n",
    "                model.covar_module.base_kernel.lengthscale.item(),\n",
    "                model.likelihood.noise.item()\n",
    "            ))\n",
    "        optimizer.step()\n",
    "\n",
    "    return model, likelihood "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Regression model\n",
      "Iter 50/50 - Loss: -0.396   lengthscale: 1.125   noise: 0.007\n",
      "Training Classification model\n",
      "Iter 50/50 - Loss: 4.470   lengthscale: 0.862   noise: 0.582\n"
     ]
    }
   ],
   "source": [
    "# generates training data\n",
    "def gen_data(num_data, seed = 2024):\n",
    "    torch.random.manual_seed(seed)\n",
    "\n",
    "    x = torch.randn(num_data,1)\n",
    "    y = torch.randn(num_data,1)\n",
    "\n",
    "    # u = torch.rand(1)\n",
    "    data_fn_obj = lambda x,y: torch.cos(2*x)*torch.cos(y) + torch.sin(x) # toy function\n",
    "    latent_fn_obj = data_fn_obj(x, y)\n",
    "    z_obj = latent_fn_obj.squeeze()\n",
    "    data_fn_cnt = lambda x, y: torch.cos(x)*torch.cos(y) - torch.sin(x)*torch.sin(y)\n",
    "    latent_fn_cnt = data_fn_cnt(x, y)\n",
    "    z_cnt = torch.heaviside(latent_fn_cnt - 0.5, torch.zeros(latent_fn_cnt.shape)).long().squeeze()\n",
    "    return torch.cat((x,y),dim=1), z_obj, z_cnt, data_fn_obj, data_fn_cnt\n",
    "\n",
    "# generate training data - y= objective values, c=class label\n",
    "train_x, train_y, train_c, genfn_obj, genfn_cnt = gen_data(50)\n",
    "\n",
    "#initialize likelihood and model - regression\n",
    "reg_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "reg_model = ExactGPModel(train_x, train_y, reg_likelihood)\n",
    "\n",
    "# initialize likelihood and model - we let the DirichletClassificationLikelihood compute the targets for us\n",
    "cls_likelihood = DirichletClassificationLikelihood(train_c, learn_additional_noise=True)\n",
    "cls_model = DirichletGPModel(train_x, cls_likelihood.transformed_targets, cls_likelihood, num_classes=cls_likelihood.num_classes)\n",
    "\n",
    "# Train regression and classification model\n",
    "training_iter = 50\n",
    "print(\"Training Regression model\")\n",
    "reg_model, reg_likelihood = train_reg_gp(reg_model, reg_likelihood, train_x, train_y, training_iter)\n",
    "print(\"Training Classification model\")\n",
    "cls_model, cls_likelihood = train_cls_gp(cls_model, cls_likelihood, train_x, training_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test data generate - uniform across [-3, 3] x [-3, 3]\n",
    "test_d1 = np.linspace(-3, 3, 20)\n",
    "test_d2 = np.linspace(-3, 3, 20)\n",
    "\n",
    "test_x1_mat, test_x2_mat = np.meshgrid(test_d1, test_d2)\n",
    "test_x1_mat, test_x2_mat = torch.Tensor(test_x1_mat), torch.Tensor(test_x2_mat)\n",
    "\n",
    "test_x = torch.cat((test_x1_mat.view(-1,1), test_x2_mat.view(-1,1)), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_acf(pred_mean, pred_std, y_max):\n",
    "    improve = y_max - pred_mean\n",
    "    z_score = np.divide(improve, pred_std + 1e-9)\n",
    "    acf = np.multiply(improve, norm.cdf(z_score)) + np.multiply(pred_std, norm.pdf(z_score))\n",
    "    return acf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\goura\\anaconda3\\envs\\gpytorch\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-06 to the diagonal\n",
      "  warnings.warn(\n",
      "c:\\Users\\goura\\anaconda3\\envs\\gpytorch\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-05 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best obj value in training so far =  tensor(-1.7209)\n",
      "top candidate indices =  tensor([184, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.423   lengthscale: 1.137   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.455   lengthscale: 0.822   noise: 0.390\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9198)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.458   lengthscale: 1.140   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.442   lengthscale: 0.777   noise: 0.256\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.494   lengthscale: 1.138   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.404   lengthscale: 0.746   noise: 0.169\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.528   lengthscale: 1.136   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.356   lengthscale: 0.727   noise: 0.128\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.561   lengthscale: 1.134   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.306   lengthscale: 0.708   noise: 0.113\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.592   lengthscale: 1.132   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.258   lengthscale: 0.693   noise: 0.109\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.622   lengthscale: 1.130   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.211   lengthscale: 0.681   noise: 0.107\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.650   lengthscale: 1.129   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.166   lengthscale: 0.671   noise: 0.104\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.676   lengthscale: 1.127   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.122   lengthscale: 0.664   noise: 0.100\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.702   lengthscale: 1.126   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.080   lengthscale: 0.658   noise: 0.095\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.726   lengthscale: 1.126   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.040   lengthscale: 0.653   noise: 0.090\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.749   lengthscale: 1.125   noise: 0.007\n",
      "Iter 50/50 - Loss: 4.001   lengthscale: 0.649   noise: 0.085\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.770   lengthscale: 1.124   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.964   lengthscale: 0.646   noise: 0.079\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([205, 204])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.791   lengthscale: 1.123   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.928   lengthscale: 0.643   noise: 0.075\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.811   lengthscale: 1.123   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.894   lengthscale: 0.641   noise: 0.070\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.830   lengthscale: 1.122   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.861   lengthscale: 0.639   noise: 0.066\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.848   lengthscale: 1.122   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.829   lengthscale: 0.638   noise: 0.062\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.865   lengthscale: 1.122   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.799   lengthscale: 0.637   noise: 0.058\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.882   lengthscale: 1.121   noise: 0.007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\goura\\anaconda3\\envs\\gpytorch\\Lib\\site-packages\\linear_operator\\utils\\cholesky.py:40: NumericalWarning: A not p.d., added jitter of 1.0e-04 to the diagonal\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50/50 - Loss: 3.770   lengthscale: 0.636   noise: 0.054\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.898   lengthscale: 1.121   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.742   lengthscale: 0.634   noise: 0.051\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.913   lengthscale: 1.121   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.716   lengthscale: 0.633   noise: 0.048\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.927   lengthscale: 1.121   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.690   lengthscale: 0.632   noise: 0.045\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.941   lengthscale: 1.120   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.665   lengthscale: 0.631   noise: 0.043\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.955   lengthscale: 1.120   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.641   lengthscale: 0.629   noise: 0.040\n",
      "======= BO Batch done ========\n",
      "best obj value in training so far =  tensor(-1.9324)\n",
      "top candidate indices =  tensor([204, 205])\n",
      "re-training the model\n",
      "Iter 50/50 - Loss: -0.967   lengthscale: 1.120   noise: 0.007\n",
      "Iter 50/50 - Loss: 3.618   lengthscale: 0.628   noise: 0.038\n",
      "======= BO Batch done ========\n"
     ]
    }
   ],
   "source": [
    "eval_budget = 50\n",
    "num_feval = 0\n",
    "bo_batch_size = 2\n",
    "\n",
    "#store BO iteration results\n",
    "obj_history = []\n",
    "top_ind_history = []\n",
    "feasibility_history = [] \n",
    "\n",
    "while num_feval < eval_budget :\n",
    "    # prediction - objective values\n",
    "    reg_model.eval()\n",
    "    reg_likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        obj_pred = reg_likelihood(reg_model(test_x))\n",
    "        obj_pred_means = obj_pred.loc\n",
    "        obj_pred_stddev = obj_pred.stddev\n",
    "    \n",
    "     #prediction - class probabilities\n",
    "    cls_model.eval()\n",
    "    cls_likelihood.eval()\n",
    "\n",
    "    with torch.no_grad(), gpytorch.settings.fast_pred_var():\n",
    "        logit_dist = cls_model(test_x)\n",
    "        logit_means = logit_dist.loc\n",
    "        logit_stddevs = logit_dist.stddev\n",
    "\n",
    "    # calculate probabilites from the logit values\n",
    "    samples = logit_dist.sample(torch.Size((256,))).exp()\n",
    "    class_probabilites = (samples / samples.sum(-2, keepdim=True)).mean(0)\n",
    "\n",
    "    best_idx = np.argmin(train_y)\n",
    "    y_best = train_y[best_idx]\n",
    "    print(\"best obj value in training so far = \", y_best)\n",
    "\n",
    "    ei = calculate_acf(obj_pred_means, obj_pred_stddev, y_best)\n",
    "    con_ei = class_probabilites[0] * ei\n",
    "\n",
    "    # top 5 candidate points from constrained ei\n",
    "    top5_ind = np.argpartition(con_ei, -bo_batch_size)[-bo_batch_size:]\n",
    "    top5_ind = top5_ind[np.argsort(con_ei[top5_ind])]\n",
    "    print(\"top candidate indices = \", top5_ind)\n",
    "\n",
    "    #locations of top candidate points\n",
    "    candidate_x = test_x[top5_ind, :]\n",
    "\n",
    "    top5_obj_vals = genfn_obj(candidate_x[:,0], candidate_x[:, 1])\n",
    "\n",
    "    num_feval += bo_batch_size\n",
    "\n",
    "    #store results\n",
    "    obj_history.append(top5_obj_vals)\n",
    "    top_ind_history.append(top5_ind)\n",
    "    feasibility_history.append(class_probabilites[0][top5_ind])\n",
    "\n",
    "    #update training data set\n",
    "    train_x = torch.cat((train_x, candidate_x), dim=0)\n",
    "    train_y = torch.cat((train_y, top5_obj_vals), dim=0)\n",
    "    train_c = torch.cat((train_c, class_probabilites.max(0)[1][top5_ind]), dim=0)\n",
    "    print('re-training the model')\n",
    "\n",
    "    #update GP\n",
    "    reg_likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
    "    reg_model = ExactGPModel(train_x, train_y, reg_likelihood)\n",
    "    cls_likelihood = DirichletClassificationLikelihood(train_c, learn_additional_noise=True)\n",
    "    cls_model = DirichletGPModel(train_x, cls_likelihood.transformed_targets, cls_likelihood, num_classes=cls_likelihood.num_classes)\n",
    "\n",
    "    reg_model, reg_likelihood = train_reg_gp(reg_model, reg_likelihood, train_x, train_y, training_iter)\n",
    "    cls_model, cls_likelihood = train_cls_gp(cls_model, cls_likelihood, train_x, training_iter)\n",
    "    print('======= BO Batch done ========')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
